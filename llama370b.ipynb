{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from HF Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Llama.from_pretrained(#\n",
    "#    repo_id=\"QuantFactory/Meta-Llama-3-70B-Instruct-GGUF\",\n",
    "#    filename=\"Meta-Llama-3-70B-Instruct.Q2_K.gguf\",\n",
    "#    n_ctx=2048,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"models/Meta-Llama-3-70B-Instruct.Q4_K_S.gguf\"\n",
    "# model_file = \"models/Llama-3.2-3B-Instruct-Q6_K_L.gguf\" #pretty good, about 4gb\n",
    "# model_file = \"models/Llama-3.2-8B.F16.gguf\" #requires more than 15gb vram\n",
    "# model_file = \"models/Llama-3.2-3B-Instruct-f16.gguf\"#8gb vram appears to be worse than the q6kl version\n",
    "model_file = \"models/CalmeRys-78B-Orpo-v0.1-IQ4_XS.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 40 key-value pairs and 1035 tensors from models/CalmeRys-78B-Orpo-v0.1-IQ4_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Calme 2.4 Rys 78b\n",
      "llama_model_loader: - kv   3:                             general.author str              = dfurman\n",
      "llama_model_loader: - kv   4:                            general.version str              = v0.1\n",
      "llama_model_loader: - kv   5:                       general.organization str              = MaziyarPanahi\n",
      "llama_model_loader: - kv   6:                           general.finetune str              = Orpo\n",
      "llama_model_loader: - kv   7:                           general.basename str              = calme-2.4-rys\n",
      "llama_model_loader: - kv   8:                         general.size_label str              = 78B\n",
      "llama_model_loader: - kv   9:                            general.license str              = mit\n",
      "llama_model_loader: - kv  10:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv  11:                  general.base_model.0.name str              = Calme 2.4 Rys 78b\n",
      "llama_model_loader: - kv  12:          general.base_model.0.organization str              = MaziyarPanahi\n",
      "llama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/MaziyarPanahi/...\n",
      "llama_model_loader: - kv  14:                               general.tags arr[str,5]       = [\"orpo\", \"qwen2\", \"sft\", \"chatml\", \"t...\n",
      "llama_model_loader: - kv  15:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  16:                           general.datasets arr[str,1]       = [\"mlabonne/orpo-dpo-mix-40k\"]\n",
      "llama_model_loader: - kv  17:                          qwen2.block_count u32              = 86\n",
      "llama_model_loader: - kv  18:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  19:                     qwen2.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv  20:                  qwen2.feed_forward_length u32              = 29568\n",
      "llama_model_loader: - kv  21:                 qwen2.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv  22:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  23:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  24:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 30\n",
      "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151644\n",
      "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151645\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/CalmeRys-78B-Orpo-v0.1-GG...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 602\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  431 tensors\n",
      "llama_model_loader: - type q5_K:   86 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_loader: - type iq4_nl:   86 tensors\n",
      "llama_model_loader: - type iq4_xs:  431 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.9308 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151646\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_layer          = 86\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 29568\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = IQ4_XS - 4.25 bpw\n",
      "llm_load_print_meta: model params     = 77.97 B\n",
      "llm_load_print_meta: model size       = 39.63 GiB (4.37 BPW) \n",
      "llm_load_print_meta: general.name     = Calme 2.4 Rys 78b\n",
      "llm_load_print_meta: BOS token        = 151644 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.91 MiB\n",
      "llm_load_tensors: offloading 20 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 20/87 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 40578.76 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  9064.53 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 5024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1295.25 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   392.50 MiB\n",
      "llama_new_context_with_model: KV self size  = 1687.75 MiB, K (f16):  843.88 MiB, V (f16):  843.88 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1300.04 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    25.82 MiB\n",
      "llama_new_context_with_model: graph nodes  = 3016\n",
      "llama_new_context_with_model: graph splits = 928\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '602', 'quantize.imatrix.file': '/models_out/CalmeRys-78B-Orpo-v0.1-GGUF/CalmeRys-78B-Orpo-v0.1.imatrix', 'general.quantization_version': '2', 'general.author': 'dfurman', 'general.size_label': '78B', 'general.type': 'model', 'general.organization': 'MaziyarPanahi', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.base_model.0.repo_url': 'https://huggingface.co/MaziyarPanahi/calme-2.4-rys-78b', 'general.version': 'v0.1', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.base_model.0.name': 'Calme 2.4 Rys 78b', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'general.base_model.0.organization': 'MaziyarPanahi', 'general.basename': 'calme-2.4-rys', 'qwen2.embedding_length': '8192', 'tokenizer.ggml.padding_token_id': '151645', 'qwen2.block_count': '86', 'general.architecture': 'qwen2', 'qwen2.context_length': '32768', 'qwen2.feed_forward_length': '29568', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.head_count': '64', 'general.license': 'mit', 'qwen2.attention.head_count_kv': '8', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.ggml.eos_token_id': '151645', 'general.finetune': 'Orpo', 'general.file_type': '30', 'general.name': 'Calme 2.4 Rys 78b', 'tokenizer.ggml.bos_token_id': '151644'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "    model_path=model_file,  # Download the model file first\n",
    "    n_ctx=5000,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=12,  # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=20,  # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.11,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   30118.46 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  120667.63 ms /    27 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4d11ccef-2a5e-4253-a020-6e8df47b8a09',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1731270562,\n",
       " 'model': 'models/CalmeRys-78B-Orpo-v0.1-IQ4_XS.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'The capital of France is Paris.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 20, 'completion_tokens': 7, 'total_tokens': 27}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Below are some blood test results for a dog.\n",
    "They contain information on the tested molecule, the detected level in the blood and the normal range of values.\n",
    "\n",
    "# BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\n",
    "Certainly, here are the blood test chemical names, their detected levels, and the corresponding normal ranges:\n",
    "\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\n",
    "7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\n",
    "8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\n",
    "9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\n",
    "10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\n",
    "11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\n",
    "12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\n",
    "13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\n",
    "14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\n",
    "15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\n",
    "16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\n",
    "17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\n",
    "18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\n",
    "19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\n",
    "20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\n",
    "21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\n",
    "22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\n",
    "23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\n",
    "24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\n",
    "25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\n",
    "26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\n",
    "27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\n",
    "28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\n",
    "29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\n",
    "30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\n",
    "31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\n",
    "32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\n",
    "33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\n",
    "34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\n",
    "35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\n",
    "36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\n",
    "37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\n",
    "38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\n",
    "39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\n",
    "40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\n",
    "41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\n",
    "42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\n",
    "43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\n",
    "44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\n",
    "45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\n",
    "46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the general prompt structure as a template\n",
    "general_structure = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question.\n",
    "The context may contain multiple question answer pairs as an example, just answer the final question provided after the context.\n",
    "If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "Question: {input}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Define the specific question as a string\n",
    "input_question = \"\"\"\n",
    "1. For each tested molecule check the detected level.\n",
    "2. For each tested molecule analyze the normal range.\n",
    "3. For each tested molecule infer whether the detected level is outside the normal range.\n",
    "4. Select and return exclusively the molecules that fall outside the normal range, citing both the etected level and the range.\n",
    "5. Provide a comprehensive health diagnosis for the dog based on detected abnormalities.\n",
    "\"\"\"\n",
    "\n",
    "# Combine everything by formatting the general structure with context and input\n",
    "llama3_prompt = general_structure.format(context=context, input=input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   30118.46 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1688 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time = 1834679.04 ms /  1849 tokens\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    llama3_prompt,\n",
    "    max_tokens=2048,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected levels for CPK, ALT, ALP, Trigliceridi, and AGP were found outside their respective normal ranges: CPK at 217 (45 - 155), ALT at 83 (10 - 60), ALP at 2780 (45 - 152), Trigliceridi at 158 (30 - 95), and AGP not detected (0,01 - 0,10). I cannot provide a comprehensive health diagnosis beyond the fact that these abnormalities may indicate potential issues with muscle and liver function, as well as lipid metabolism. Further veterinary examination is recommended. <|eot_id|><|start_of_text|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(llama3_prompt) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the general prompt structure as a template\n",
    "general_structure = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "[INST]\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question.\n",
    "The context may contain multiple question answer pairs as an example, just answer the final question provided after the context.\n",
    "If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "[\\INST]\n",
    "{context}\n",
    "Question: {input}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Define the specific question as a string\n",
    "input_question = \"\"\"\n",
    "1. For each tested molecule check the detected level.\n",
    "2. For each tested molecule analyze the normal range.\n",
    "3. For each tested molecule infer whether the detected level is outside the normal range.\n",
    "4. Select and return exclusively the molecules that fall outside the normal range, citing both the etected level and the range.\n",
    "5. Provide a comprehensive health diagnosis for the dog based on detected abnormalities.\n",
    "\"\"\"\n",
    "\n",
    "# Combine everything by formatting the general structure with context and input\n",
    "llama3_prompt = general_structure.format(context=context, input=input_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 22 prefix-match hit, remaining 1672 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   30118.46 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1672 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time = 2473721.32 ms /  1887 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-552971f9-79e8-43e6-8f8c-6ec3e4b99a5e', 'object': 'text_completion', 'created': 1731272521, 'model': 'models/CalmeRys-78B-Orpo-v0.1-IQ4_XS.gguf', 'choices': [{'text': '\\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n[INST]\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question.\\nThe context may contain multiple question answer pairs as an example, just answer the final question provided after the context.\\nIf you don\\'t know the answer just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\n[\\\\INST]\\n\\nBelow are some blood test results for a dog.\\nThey contain information on the tested molecule, the detected level in the blood and the normal range of values.\\n\\n# BLOOD_TEST_DIAGNOSIS_DOCUMENT\\n\\nCertainly, here are the blood test chemical names, their detected levels, and the corresponding normal ranges:\\n\\n1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\\n2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\\n3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\\n4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\\n5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\\n6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\\n7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\\n8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\\n9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\\n10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\\n11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\\n12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\\n13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\\n14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\\n15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\\n16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\\n17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\\n18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\\n19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\\n20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\\n21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\\n22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\\n23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\\n24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\\n25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\\n26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\\n27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\\n28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\\n29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\\n30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\\n31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\\n32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\\n33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\\n34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\\n35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\\n36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\\n37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\\n38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\\n39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\\n40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\\n41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\\n42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\\n43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\\n44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\\n45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\\n46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\\nQuestion: \\n1. For each tested molecule check the detected level.\\n2. For each tested molecule analyze the normal range.\\n3. For each tested molecule infer whether the detected level is outside the normal range.\\n4. Select and return exclusively the molecules that fall outside the normal range, citing both the etected level and the range.\\n5. Provide a comprehensive health diagnosis for the dog based on detected abnormalities.\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n[INST]\\nThe molecules outside the normal range include: CPK (217, Normal range: 45 - 155), ALT (83, Normal range: 10 - 60), ALP (2780, Normal range: 45 - 152), Trigliceridi (158, Normal range: 30 - 95), Albumine (3.26, Normal range: 2.6 - 3.8), Globuline (3.90, Normal range: 2.6 - 4.5), Rapp. Alb/Glob (0.84, Normal range: 0.84 - 1.91), and Amilasi (836, Normal range: 200 - 1900). The dog may show signs of muscle damage, liver disease, possible bone issues, pancreas abnormalities, and potential issues with protein balance.\\n[\\\\INST]<|eot_id|>', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 1694, 'completion_tokens': 215, 'total_tokens': 1909}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    llama3_prompt,\n",
    "    max_tokens=2048,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]\n",
      "The molecules outside the normal range include: CPK (217, Normal range: 45 - 155), ALT (83, Normal range: 10 - 60), ALP (2780, Normal range: 45 - 152), Trigliceridi (158, Normal range: 30 - 95), Albumine (3.26, Normal range: 2.6 - 3.8), Globuline (3.90, Normal range: 2.6 - 4.5), Rapp. Alb/Glob (0.84, Normal range: 0.84 - 1.91), and Amilasi (836, Normal range: 200 - 1900). The dog may show signs of muscle damage, liver disease, possible bone issues, pancreas abnormalities, and potential issues with protein balance.\n",
      "[\\INST]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(llama3_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt focused on detecting only abnormal values\n",
    "few_shot_prompt = \"\"\"\n",
    "### Example 1\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "\n",
    "Expected Output:\n",
    "Abnormal Results:\n",
    "1. CPK: High (Detected level 217, above normal range of 45 - 155).\n",
    "3. ALT: High (Detected level 83, above normal range of 10 - 60).\n",
    "\n",
    "### Example 2\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 0.01, Normal range: 0.1 - 0.13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0.10, Normal range: 0.10 - 0.44\n",
    "\n",
    "Expected Output:\n",
    "Abnormal Results:\n",
    "4. ALP: High (Detected level 2780, above normal range of 45 - 152).\n",
    "5. GGT: Low (Detected level 0.01, below normal range of 0.1 - 0.13).\n",
    "\n",
    "### Your Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_input = \"\"\"Analyze the following blood test results and report only the abnormal values.\\n\"\"\"\n",
    "\n",
    "context = \"\"\"# BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\n",
    "Certainly, here are the blood test chemical names, their detected levels, and the corresponding normal ranges:\n",
    "\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\n",
    "7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\n",
    "8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\n",
    "9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\n",
    "10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\n",
    "11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\n",
    "12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\n",
    "13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\n",
    "14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\n",
    "15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\n",
    "16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\n",
    "17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\n",
    "18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\n",
    "19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\n",
    "20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\n",
    "21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\n",
    "22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\n",
    "23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\n",
    "24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\n",
    "25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\n",
    "26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\n",
    "27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\n",
    "28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\n",
    "29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\n",
    "30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\n",
    "31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\n",
    "32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\n",
    "33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\n",
    "34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\n",
    "35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\n",
    "36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\n",
    "37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\n",
    "38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\n",
    "39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\n",
    "40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\n",
    "41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\n",
    "42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\n",
    "43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\n",
    "44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\n",
    "45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\n",
    "46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\"\"\"\n",
    "# Combine the few-shot prompt with the user's input\n",
    "full_prompt = few_shot_prompt + context + user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 1810 prompt tokens to eval\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    full_prompt,\n",
    "    max_tokens=2048,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(full_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought prompt with an example\n",
    "cot_prompt = \"\"\"\n",
    "Analyze the following blood test results, identify abnormal values, and explain the reasoning step by step.\n",
    "\n",
    "### Example 1\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "1. AST (IU/L): Detected level: 50, Normal range: 10 - 45\n",
    "2. ALT (IU/L): Detected level: 25, Normal range: 10 - 60\n",
    "3. ALP (IU/L): Detected level: 200, Normal range: 45 - 152\n",
    "4. Glucosio (mg/dl): Detected level: 65, Normal range: 70 - 110\n",
    "5. STI (IU/L): Detected level: -, Normal range: 10 - 100\n",
    "\n",
    "\n",
    "Let's think step by step:\n",
    "1. AST: Detected level is 50. The normal range is 10 - 45. Since 50 is higher than 45, AST is abnormal (High).\n",
    "2. ALT: Detected level is 25. The normal range is 10 - 60. Since 25 is within this range, ALT is normal.\n",
    "3. ALP: Detected level is 200. The normal range is 45 - 152. Since 200 is higher than 152, ALP is abnormal (High).\n",
    "4. Glucosio: Detected level is 65. The normal range is 70 - 110. Since 65 is lower than 70, Glucosio is abnormal (Low).\n",
    "5. STI: Detected level is -. The normal range is 10 - 100. This molecule was not tested.\n",
    "Abnormal Results:\n",
    "1. AST: High (Detected level 50, above normal range of 10 - 45).\n",
    "3. ALP: High (Detected level 200, above normal range of 45 - 152).\n",
    "4. Glucosio: Low (Detected level 65, below normal range of 70 - 110).\n",
    "\n",
    "### New Input:\n",
    "\"\"\"\n",
    "\n",
    "# Combine prompt with new input\n",
    "full_prompt = cot_prompt + user_input + context + \"\\nLet's think step by step:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    full_prompt,\n",
    "    max_tokens=4096,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(full_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Prompt Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more detailed system prompt to guide the LLM effectively\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are analyzing blood test results. Your task is to detect abnormalities and provide a health diagnosis.\n",
    "For each abnormal result, provide the chemical name, its detected level, and indicate whether it's above or below the normal range.\n",
    "An abnormal value is defined as a value which is below the minimum normal range or above the maximum normal range.\n",
    "If no abnormal values are found, respond with \"All values are within the normal range.\"\n",
    "Do not include any irrelevant details or chemicals that are within the normal range.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Function to generate the prompt using the enhanced system and user-specific inputs\n",
    "def generate_prompt(\n",
    "    context: str, question: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Question is focused and clear\n",
    "question = \"Extract all abnormal chemical names with their detected levels and indicate if they are above or below the normal range.\"\n",
    "\n",
    "# Generate the refined prompt\n",
    "final_prompt = generate_prompt(context=context, question=question)\n",
    "\n",
    "# Print the final prompt to see how it looks\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    final_prompt,\n",
    "    max_tokens=4096,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(full_prompt) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
