{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Give an analyisis of these blood test results, give insight on the general health condition as well as highlight potential health problems by analyzing blood test values that fall out of the normal range\n",
    "BLOOD TEST RESULTS:\n",
    "\n",
    "PROFILO BIOCHIMICO\n",
    "\n",
    "RIFERIM. DEL Cane* RIFERIM. DEL Cane*\n",
    "CPK (IU/L): 217 45 155 Cloro corr. (mEq/L): 108,04 98 118\n",
    "AST (IU/L): 36 10 45 HCO3 (mEq/L): 21 17,0 25,2\n",
    "ALT (IU/L): 83 10 60 Div. anionico : 23 12,0 24,0\n",
    "ALP (IU/L): 2780 45 152 Osmolarita cal. (mOsm): 322 314 335\n",
    "GGT (IU/L): 3,0 0,1 13 Ferro (ug/dl): 114 100 280\n",
    "Bilirubina Tot (mg/dl): 0,10 0,10 0,44 UIBC (ug/dl): 330 150 350\n",
    "Proteine Tot (g/dl): 7,2 5,8 8,0 TIBC (peg/dl): 444 300 510\n",
    "Albumine (g/dl): 3,26 2,6 3,8 Saturazione (%): 25,68 30 60\n",
    "Globuline (g/dl): 3,90 2,6 4,5 21 78\n",
    "Rapp. Alb/Glob : 0,84 0,84 1,91 20 60\n",
    "Colesterolo (mg/dl): 201 120 300 0,3 9\n",
    "Trigliceridi (mg/dl): 158 30 95 0,6 30\n",
    "Amilasi (IU/L): 836 200 1900\n",
    "Lipasi (IU/L): 64 10 350 0,02 0,65\n",
    "UREA (mg/dl): 21 15 45 Colinesterasi (IU/L): 2282 3350 6550\n",
    "CREA (mg/dl): 0,69 0,60 1,80 30 398\n",
    "Glucosio (mg/dl): 88 70 110 188 351\n",
    "Calcio (mg/dl): 9,80 8,0 12,0 0,2 9,0\n",
    "0,2 1 2,5 4,95\n",
    "Fosforo (mg/dl): 3,90 2,5 5,6 0,01 0,10\n",
    "Sodio (mEq/L): 150 144 155\n",
    "Potassio (mEq/L): 4,9 3,3 5,4\n",
    "Rapp. Na/K : 30,61 >27\n",
    "Cloro (mEq/L): 111 100 118\n",
    "Magnesio (mg/dl): 1,50 1,60 2,48\n",
    "0,97 1,34\n",
    "SIERO OPALESCENTE PCR NON\n",
    "ATTENDIBILE\n",
    "ALP RIPETUTA E CONFERMATA\"\"\"\n",
    "\n",
    "system_message = \"You are an experienced dog veterinary. You will help the dog owner understand the health of his dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "    model_path=r\"capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\",  # Download the model file first\n",
    "    n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=8,  # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=35,  # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 23 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     216.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1110.30 ms /    63 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "    \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\",  # Prompt\n",
    "    max_tokens=512,  # Generate up to 512 tokens\n",
    "    stop=[\n",
    "        \"</s>\"\n",
    "    ],  # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "    echo=True,  # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('id', 'cmpl-22edd41f-1035-4ed3-99f2-b4df59cd7767'), ('object', 'text_completion'), ('created', 1730739657), ('model', 'capybarahermes-2.5-mistral-7b.Q4_K_S.gguf'), ('choices', [{'text': '<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\nHere\\'s a step-by-step guide to create a new project in Eclipse:\\n\\n1. Launch Eclipse: Open Eclipse IDE on your computer.\\n\\n2. Create a new workspace: Eclipse allows you to work in multiple workspaces, and each workspace can contain different projects. To create a new workspace, go to the \"File\" menu and select \"Switch Workspace\". Then choose \"New Workspace\". Choose the location where you want to create your new workspace, and click on \"Create\".\\n\\n3. Start a new project: Once you\\'ve created a new workspace, you can start a new project. Go to the \"File\" menu again and select \"New\" followed by \"Project\".\\n\\n4. Choose a project type: A dialog box will appear, and you\\'ll need to choose the project type. The type of project you select will determine the files that Eclipse creates when setting up your new project. Common types include Java, Android, and Maven project types. For this example, let\\'s select \"Java Project\".\\n\\n5. Name your project: In the dialog box, you\\'ll also need to enter a name for your project in the \"Project name\" field.\\n\\n6. Set the location: You can also set the location where you want your project to be created. The location is typically a folder within your workspace.\\n\\n7. Finish and create project: Once you\\'ve completed filling out the details, click \"Finish\". Eclipse will then create the new project based on the type and location you specified.\\n\\n8. Configure the project: If you selected a project type that has additional settings, you may need to configure the project. Eclipse will guide you through the steps you need to take.\\n\\nNow you\\'ve successfully created a new project in Eclipse! You can start adding files, such as Java classes and other resources, to your new project and begin your development work.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}]), ('usage', {'prompt_tokens': 24, 'completion_tokens': 415, 'total_tokens': 439})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"you are an expert veterinary. You are to help the user understand what is wrong with his dog.\"\"\"\n",
    "prompt = (\n",
    "    \"\"\"The dog has a large bmi with noticeable weakness in the bone density levels.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     216.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     569.31 ms /    50 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "    \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\",  # Prompt\n",
    "    max_tokens=512,  # Generate up to 512 tokens\n",
    "    stop=[\n",
    "        \"</s>\"\n",
    "    ],  # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "    echo=True,  # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-bb9f23c6-d319-4d05-bd15-ad715d675ab8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1730741300,\n",
       " 'model': 'capybarahermes-2.5-mistral-7b.Q4_K_S.gguf',\n",
       " 'choices': [{'text': '<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{prompt} is not defined in your input. Please provide a specific prompt or question that you would like me to address.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 24, 'completion_tokens': 27, 'total_tokens': 51}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-04 18:37:45.345464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730741865.361862   68861 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730741865.366923   68861 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 18:37:45.384548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at Mistral-7B-OpenOrca-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Mistral-7B-OpenOrca-GPTQ\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mistral-7B-OpenOrca-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Orcas were not known to be drawn to mistral energy, but they were seen recently 100 miles off the coast of Brazil, where the Brazilian Navy was conducting a training exercise.\n",
      "\n",
      "The Brazilian Navy was conducting a training exercise in the Atlantic Ocean when they noticed something unusual. They saw a group of orcas swimming in the area where the navy was conducting its exercises.\n",
      "\n",
      "Orcas, also known as killer whales, are not typically known to be drawn to mistral energy. However, the Brazilian Navy was using a device called a mistral, which is a type of sonar system used to detect underwater objects.\n",
      "\n",
      "The orcas were seen swimming 100 miles off the coast of Brazil, where the mistral energy was being used. The navy noticed the orcas and decided to stop their exercises to ensure the safety of the marine mammals.\n",
      "\n",
      "The orcas were observed swimming in the area for a short time before they swam away. The navy then resumed their training exercises.\n",
      "\n",
      "This encounter between the Brazilian Navy and the orcas highlights the potential impact of sonar systems on marine life. While the navy was able to stop their exercises to ensure the safety of the orcas, it is important to consider the potential effects of such technology on other marine species.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Orcas were not known to be drawn to mistral energy, but they were seen recently \",\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95\n",
    ")\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "you are an expert veterinary. You are to help the user understand what is wrong with his dog.<|im_end|><|im_start|> user\n",
      "The dog has a large bmi with noticeable weakness in the bone density levels.<|im_end|><|im_start|> assistant\n",
      " A dog with a high Body Condition Score (BCS) and weak bone density might be experiencing issues related to his weight and overall health. It is crucial to visit your veterinarian for a thorough examination and advice on how to improve your pet's condition.\n",
      "\n",
      "There could be several reasons why your dog has a high BCS and low bone density. Here are some possible causes:\n",
      "\n",
      "1. Obesity: An overweight dog with a high BCS may have excessive fat accumulation, which puts additional stress on the bones and can eventually lead to bone density loss. It may also cause osteoarthritis and other joint problems.\n",
      "\n",
      "2. Nutrition: Poor nutrition or an imbalanced diet can affect bone growth and density. A diet lacking in calcium, phosphorus, and vitamin D can contribute to bone weakening in growing puppies and adult dogs.\n",
      "\n",
      "3. Medical conditions: Various medical conditions such as hypothyroidism, Cush\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"you are an expert veterinary. You are to help the user understand what is wrong with his dog.\"\"\"\n",
    "prompt = (\n",
    "    \"\"\"The dog has a large bmi with noticeable weakness in the bone density levels.\"\"\"\n",
    ")\n",
    "\n",
    "prefix = \"<|im_start|>\"\n",
    "suffix = \"<|im_end|>\\n\"\n",
    "sys_format = prefix + \"system\\n\" + system_message + suffix\n",
    "user_format = prefix + \"user\\n\" + prompt + suffix\n",
    "assistant_format = prefix + \"assistant\\n\"\n",
    "input_text = sys_format + user_format + assistant_format\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=256,\n",
    "    temperature=1.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(\n",
    "    device\n",
    ")\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "You are an expert veterinary. Give an analyisis of these blood test results, give insight on the general health condition as well as highlight potential health problems by analyzing blood test values that fall out of the normal range<|im_end|><|im_start|> user\n",
      "\n",
      "BLOOD TEST RESULTS:\n",
      "\n",
      "PROFILO BIOCHIMICO\n",
      "\n",
      "RIFERIM. DEL Cane* RIFERIM. DEL Cane*\n",
      "CPK (IU/L): 217 45 155 Cloro corr. (mEq/L): 108,04 98 118\n",
      "AST (IU/L): 36 10 45 HCO3 (mEq/L): 21 17,0 25,2\n",
      "ALT (IU/L): 83 10 60 Div. anionico : 23 12,0 24,0\n",
      "ALP (IU/L): 2780 45 152 Osmolarita cal. (mOsm): 322 314 335\n",
      "GGT (IU/L): 3,0 0,1 13 Ferro (ug/dl): 114 100 280\n",
      "Bilirubina Tot (mg/dl): 0,10 0,10 0,44 UIBC (ug/dl): 330 150 350\n",
      "Proteine Tot (g/dl): 7,2 5,8 8,0 TIBC (peg/dl): 444 300 510\n",
      "Albumine (g/dl): 3,26 2,6 3,8 Saturazione (%): 25,68 30 60\n",
      "Globuline (g/dl): 3,90 2,6 4,5 21 78\n",
      "Rapp. Alb/Glob : 0,84 0,84 1,91 20 60\n",
      "Colesterolo (mg/dl): 201 120 300 0,3 9\n",
      "Trigliceridi (mg/dl): 158 30 95 0,6 30\n",
      "Amilasi (IU/L): 836 200 1900\n",
      "Lipasi (IU/L): 64 10 350 0,02 0,65\n",
      "UREA (mg/dl): 21 15 45 Colinesterasi (IU/L): 2282 3350 6550\n",
      "CREA (mg/dl): 0,69 0,60 1,80 30 398\n",
      "Glucosio (mg/dl): 88 70 110 188 351\n",
      "Calcio (mg/dl): 9,80 8,0 12,0 0,2 9,0\n",
      "0,2 1 2,5 4,95\n",
      "Fosforo (mg/dl): 3,90 2,5 5,6 0,01 0,10\n",
      "Sodio (mEq/L): 150 144 155\n",
      "Potassio (mEq/L): 4,9 3,3 5,4\n",
      "Rapp. Na/K : 30,61 >27\n",
      "Cloro (mEq/L): 111 100 118\n",
      "Magnesio (mg/dl): 1,50 1,60 2,48\n",
      "0,97 1,34\n",
      "SIERO OPALESCENTE PCR NON\n",
      "ATTENDIBILE\n",
      "ALP RIPETUTA E CONFERMATA<|im_end|><|im_start|> assistant\n",
      " Below I have analyzed various blood test values. Some numbers may not match the previous answer because the provided units differ (i.e., IU/L vs. u/dl), but I've provided the correct normal ranges based on the units mentioned:\n",
      "\n",
      "1. CPK (IU/L): 217, 45, 155 (Normal range: 50-195) - The level is borderline above the normal range. \n",
      "\n",
      "2. Chloride (mEq/L): 108.04, 98, 118 (Normal range: 96-106) - The level is within the normal range.\n",
      "\n",
      "3. AST (IU/L): 36, 10, 45 (Normal range: 10-30) - The level is normal.\n",
      "\n",
      "4. ALT (IU/L): 83, 10, 60 (Normal range: 10-30) - The level is within the normal range.\n",
      "\n",
      "5. Alb/Glob: 0.84, 0.84, 1.91 (Normal range: 0.6-1.1 for females, 0.8-1.2 for males) - The levels are within the normal range for females and above the range for males.\n",
      "\n",
      "6. Albumin (g/dl): 3.26, 2.6, 3.8 (Normal range: 3.5-5.0) - The level is within the normal range.\n",
      "\n",
      "7. Colesterolo (mg/dl): 201, 120, 300 (Normal range: 70-190) - The level is borderline outside the normal range.\n",
      "\n",
      "8. Glucose (mg/dl): 88, 70, 110 (Normal range: 70-100) - The level is within the normal range.\n",
      "\n",
      "9. Calcium (mg/dl): 9.80, 8.0, 12.0 (Normal range: 8.5-10.2) - The level is within the normal range.\n",
      "\n",
      "10. Potassium (mEq/L): 4.9, 3.3, 5.4 (Normal range: 3.5-5.0) - The level is within the normal range.\n",
      "\n",
      "11. Magnesium (mg/dl): 1.50, 1.60, 2.48 (Normal range: 1.5-2.5) - The level is within the normal range.\n",
      "\n",
      "12. Ratio of Sodium to Potassium (Na/K): 30.61, >27 - The value is above the normal range indicating potential abnormalities.\n",
      "\n",
      "No abnormalities were found in the rest of the listed tests. Based on these results, the overall health condition of the dog seems normal. However, certain values, such as CPK, Glucose, and Cholesterol, suggest that some monitoring and precautionary measures are needed. For instance, monitoring calcium and kidney function may be important due to higher phosphorus levels (3.90 u/dl). Additional testing, along with regular veterinary check-ups, may be necessary to ensure the health of your dog.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"You are an expert veterinary. Give an analyisis of these blood test results, give insight on the general health condition as well as highlight potential health problems by analyzing blood test values that fall out of the normal range\"\"\"\n",
    "prompt = \"\"\"\n",
    "BLOOD TEST RESULTS:\n",
    "\n",
    "PROFILO BIOCHIMICO\n",
    "\n",
    "RIFERIM. DEL Cane* RIFERIM. DEL Cane*\n",
    "CPK (IU/L): 217 45 155 Cloro corr. (mEq/L): 108,04 98 118\n",
    "AST (IU/L): 36 10 45 HCO3 (mEq/L): 21 17,0 25,2\n",
    "ALT (IU/L): 83 10 60 Div. anionico : 23 12,0 24,0\n",
    "ALP (IU/L): 2780 45 152 Osmolarita cal. (mOsm): 322 314 335\n",
    "GGT (IU/L): 3,0 0,1 13 Ferro (ug/dl): 114 100 280\n",
    "Bilirubina Tot (mg/dl): 0,10 0,10 0,44 UIBC (ug/dl): 330 150 350\n",
    "Proteine Tot (g/dl): 7,2 5,8 8,0 TIBC (peg/dl): 444 300 510\n",
    "Albumine (g/dl): 3,26 2,6 3,8 Saturazione (%): 25,68 30 60\n",
    "Globuline (g/dl): 3,90 2,6 4,5 21 78\n",
    "Rapp. Alb/Glob : 0,84 0,84 1,91 20 60\n",
    "Colesterolo (mg/dl): 201 120 300 0,3 9\n",
    "Trigliceridi (mg/dl): 158 30 95 0,6 30\n",
    "Amilasi (IU/L): 836 200 1900\n",
    "Lipasi (IU/L): 64 10 350 0,02 0,65\n",
    "UREA (mg/dl): 21 15 45 Colinesterasi (IU/L): 2282 3350 6550\n",
    "CREA (mg/dl): 0,69 0,60 1,80 30 398\n",
    "Glucosio (mg/dl): 88 70 110 188 351\n",
    "Calcio (mg/dl): 9,80 8,0 12,0 0,2 9,0\n",
    "0,2 1 2,5 4,95\n",
    "Fosforo (mg/dl): 3,90 2,5 5,6 0,01 0,10\n",
    "Sodio (mEq/L): 150 144 155\n",
    "Potassio (mEq/L): 4,9 3,3 5,4\n",
    "Rapp. Na/K : 30,61 >27\n",
    "Cloro (mEq/L): 111 100 118\n",
    "Magnesio (mg/dl): 1,50 1,60 2,48\n",
    "0,97 1,34\n",
    "SIERO OPALESCENTE PCR NON\n",
    "ATTENDIBILE\n",
    "ALP RIPETUTA E CONFERMATA\"\"\"\n",
    "prefix = \"<|im_start|>\"\n",
    "suffix = \"<|im_end|>\\n\"\n",
    "sys_format = prefix + \"system\\n\" + system_message + suffix\n",
    "user_format = prefix + \"user\\n\" + prompt + suffix\n",
    "assistant_format = prefix + \"assistant\\n\"\n",
    "input_text = sys_format + user_format + assistant_format\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=2048,\n",
    "    temperature=1.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(\n",
    "    device\n",
    ")\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
