{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from HF Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Llama.from_pretrained(#\n",
    "#    repo_id=\"QuantFactory/Meta-Llama-3-70B-Instruct-GGUF\",\n",
    "#    filename=\"Meta-Llama-3-70B-Instruct.Q2_K.gguf\",\n",
    "#    n_ctx=2048,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file = \"models/Meta-Llama-3-70B-Instruct.Q4_K_S.gguf\"\n",
    "model_file = \"../models/Llama-3.2-3B-Instruct-Q6_K_L.gguf\"  # pretty good, about 4gb\n",
    "# model_file = \"models/Llama-3.2-8B.F16.gguf\" #requires more than 15gb vram\n",
    "# model_file = \"models/Llama-3.2-3B-Instruct-f16.gguf\"#8gb vram appears to be worse than the q6kl version\n",
    "# model_file = \"models/CalmeRys-78B-Orpo-v0.1-IQ4_XS.gguf\"  # somehow does not use gpu? pretty good results but 30min for generation.\n",
    "# model_file = \"models/CalmeRys-78B-Orpo-v0.1.i1-IQ3_S.gguf\" #uses 10Gb with 20/87 layers to GPU, best performer for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from ../models/Llama-3.2-3B-Instruct-Q6_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llama_model_loader: - type q6_K:  196 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 2.54 GiB (6.80 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   399.23 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2604.91 MiB\n",
      "............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 5024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   549.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  549.50 MiB, K (f16):  274.75 MiB, V (f16):  274.75 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   269.32 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.82 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'general.license': 'llama3.2', 'llama.attention.value_length': '128', 'general.size_label': '3B', 'general.type': 'model', 'quantize.imatrix.chunks_count': '125', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'tokenizer.ggml.bos_token_id': '128000', 'general.basename': 'Llama-3.2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.block_count': '28', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.finetune': 'Instruct', 'general.file_type': '18', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'llama.rope.dimension_count': '128', 'quantize.imatrix.file': '/models_out/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'llama.attention.head_count_kv': '8', 'quantize.imatrix.entries_count': '196'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "    model_path=model_file,  # Download the model file first\n",
    "    n_ctx=5000,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=12,  # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=87,  # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.11,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =      55.56 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.00 ms /    49 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4da4a7c3-70c0-4b1e-8d55-8070045f5908',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1731879824,\n",
       " 'model': '../models/Llama-3.2-3B-Instruct-Q6_K_L.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'The capital of France is Paris.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 42, 'completion_tokens': 7, 'total_tokens': 49}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Below are some blood test results for a dog.\n",
    "They contain information on the tested molecule, the detected level in the blood and the normal range of values.\n",
    "\n",
    "# BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\n",
    "Certainly, here are the blood test chemical names, their detected levels, and the corresponding normal ranges:\n",
    "\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\n",
    "7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\n",
    "8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\n",
    "9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\n",
    "10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\n",
    "11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\n",
    "12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\n",
    "13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\n",
    "14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\n",
    "15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\n",
    "16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\n",
    "17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\n",
    "18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\n",
    "19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\n",
    "20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\n",
    "21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\n",
    "22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\n",
    "23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\n",
    "24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\n",
    "25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\n",
    "26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\n",
    "27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\n",
    "28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\n",
    "29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\n",
    "30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\n",
    "31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\n",
    "32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\n",
    "33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\n",
    "34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\n",
    "35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\n",
    "36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\n",
    "37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\n",
    "38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\n",
    "39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\n",
    "40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\n",
    "41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\n",
    "42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\n",
    "43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\n",
    "44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\n",
    "45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\n",
    "46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the general prompt structure as a template\n",
    "general_structure = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question.\n",
    "The context may contain multiple question answer pairs as an example, just answer the final question provided after the context.\n",
    "If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "Question: {input}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Define the specific question as a string\n",
    "input_question = \"\"\"\n",
    "1. For each tested molecule check the detected level.\n",
    "2. For each tested molecule analyze the normal range.\n",
    "3. For each tested molecule infer whether the detected level is outside the normal range.\n",
    "4. Select and return exclusively the molecules that fall outside the normal range, citing both the etected level and the range.\n",
    "5. Provide a comprehensive health diagnosis for the dog based on detected abnormalities.\n",
    "\"\"\"\n",
    "\n",
    "# Combine everything by formatting the general structure with context and input\n",
    "llama3_prompt = general_structure.format(context=context, input=input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 1479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      55.56 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1833 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   28698.74 ms /  3312 tokens\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "    llama3_prompt,\n",
    "    max_tokens=None,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I will answer the questions and provide the required information.\n",
      "\n",
      "The detected levels and normal ranges for each molecule are:\n",
      "\n",
      "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
      "\t* Out of range: High\n",
      "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
      "\t* In range: Low\n",
      "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
      "\t* Out of range: High\n",
      "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
      "\t* Out of range: High\n",
      "5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\n",
      "\t* In range: Low\n",
      "6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\n",
      "\t* In range: Low\n",
      "7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\n",
      "\t* In range: High\n",
      "8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\n",
      "\t* In range: High\n",
      "9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\n",
      "\t* In range: High\n",
      "10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\n",
      "\t* In range: High\n",
      "11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\n",
      "\t* In range: High\n",
      "12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\n",
      "\t* In range: High\n",
      "13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\n",
      "\t* Out of range: High\n",
      "14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\n",
      "\t* In range: Low\n",
      "15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\n",
      "\t* In range: High\n",
      "16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\n",
      "\t* In range: High\n",
      "17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\n",
      "\t* In range: High\n",
      "18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\n",
      "\t* Out of range: High\n",
      "19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\n",
      "\t* No data\n",
      "20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\n",
      "\t* In range: High\n",
      "21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\n",
      "\t* High\n",
      "22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\n",
      "\t* High\n",
      "23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\n",
      "\t* In range: High\n",
      "24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\n",
      "\t* In range: High\n",
      "25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\n",
      "\t* In range: High\n",
      "26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\n",
      "\t* In range: High\n",
      "27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\n",
      "\t* In range: High\n",
      "28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\n",
      "\t* In range: High\n",
      "29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\n",
      "\t* Out of range: High\n",
      "30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\n",
      "\t* Out of range: High\n",
      "31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\n",
      "\t* In range: High\n",
      "32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\n",
      "\t* No data\n",
      "33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\n",
      "\t* No data\n",
      "34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\n",
      "\t* In range: Low\n",
      "35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\n",
      "\t* In range: Low\n",
      "36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\n",
      "\t* No data\n",
      "37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\n",
      "\t* Out of range: High\n",
      "38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\n",
      "\t* No data\n",
      "39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\n",
      "\t* No data\n",
      "40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\n",
      "\t* No data\n",
      "41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\n",
      "\t* No data\n",
      "42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\n",
      "\t* No data\n",
      "43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\n",
      "\t* No data\n",
      "44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\n",
      "\t* No data\n",
      "45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\n",
      "\t* In range: High\n",
      "46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\n",
      "\t* In range: High\n",
      "\n",
      "The molecules outside the normal range are:\n",
      "\n",
      "* CPK (217 IU/L)\n",
      "* ALT (83 IU/L)\n",
      "* ALP (2780 IU/L)\n",
      "* Amilasi (836 IU/L)\n",
      "* UBC (330 μg/dl)\n",
      "* TIBC (444 μg/dl)\n",
      "* Cistatina (2282 mg/L)\n",
      "* Rapp. Na/K (30.61)\n",
      "* BUN (no data for BUN, but UREA is high at 21 mg/dl)\n",
      "\n",
      "The health diagnosis based on these abnormalities is:\n",
      "\n",
      "* The dog has liver damage and inflammation, indicated by high levels of ALP and ALP.\n",
      "* The dog has kidney damage and inflammation, indicated by high levels of UBC, TIBC, and low levels of Protein Tot and Albumine.\n",
      "* The dog has high levels of lipase, which can indicate pancreatitis.\n",
      "* The dog has high levels of sodium, potassium, and chloride, which can indicate a hormonal imbalance or a metabolic disorder.\n",
      "* The dog has high levels of calcium, which can indicate hyperparathyroidism.\n",
      "\n",
      "Please note that this is a summary of the abnormalities and is not a definitive diagnosis. It is recommended to consult with a veterinarian to confirm the diagnosis and provide a proper treatment plan.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(llama3_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt focused on detecting only abnormal values\n",
    "few_shot_prompt = \"\"\"\n",
    "### Example 1\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "\n",
    "Expected Output:\n",
    "Abnormal Results:\n",
    "1. CPK: High (Detected level 217, above normal range of 45 - 155).\n",
    "3. ALT: High (Detected level 83, above normal range of 10 - 60).\n",
    "\n",
    "### Example 2\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 0.01, Normal range: 0.1 - 0.13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0.10, Normal range: 0.10 - 0.44\n",
    "\n",
    "Expected Output:\n",
    "Abnormal Results:\n",
    "4. ALP: High (Detected level 2780, above normal range of 45 - 152).\n",
    "5. GGT: Low (Detected level 0.01, below normal range of 0.1 - 0.13).\n",
    "\n",
    "### Your Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_input = \"\"\"Analyze the following blood test results and report only the abnormal values.\\n\"\"\"\n",
    "\n",
    "context = \"\"\"# BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "\n",
    "Certainly, here are the blood test chemical names, their detected levels, and the corresponding normal ranges:\n",
    "\n",
    "1. CPK (IU/L): Detected level: 217, Normal range: 45 - 155\n",
    "2. AST (IU/L): Detected level: 36, Normal range: 10 - 45\n",
    "3. ALT (IU/L): Detected level: 83, Normal range: 10 - 60\n",
    "4. ALP (IU/L): Detected level: 2780, Normal range: 45 - 152\n",
    "5. GGT (IU/L): Detected level: 3,0, Normal range: 0,1 - 0,13\n",
    "6. Bilirubina Tot (mg/dl): Detected level: 0,10, Normal range: 0,10 - 0,44\n",
    "7. Proteine Tot (g/dl): Detected level: 7,2, Normal range: 5,8 - 8,0\n",
    "8. Albumine (g/dl): Detected level: 3,26, Normal range: 2,6 - 3,8\n",
    "9. Globuline (g/dl): Detected level: 3,90, Normal range: 2,6 - 4,5\n",
    "10. Rapp. Alb/Glob: Detected level: 0,84, Normal range: 0,84 - 1,91\n",
    "11. Colesterolo (mg/dl): Detected level: 201, Normal range: 120 - 300\n",
    "12. Trigliceridi (mg/dl): Detected level: 158, Normal range: 30 - 95\n",
    "13. Amilasi (IU/L): Detected level: 836, Normal range: 200 - 1900\n",
    "14. Lipasi (IU/L): Detected level: 64, Normal range: 10 - 350\n",
    "15. UREA (mg/dl): Detected level: 21, Normal range: 15 - 45\n",
    "16. CREA (mg/dl): Detected level: 0,69, Normal range: 0,60 - 1,80\n",
    "17. Glucosio (mg/dl): Detected level: 88, Normal range: 70 - 110\n",
    "18. Calcio (mg/dl): Detected level: 9,80, Normal range: 8,0 - 12,0\n",
    "19. Acido Urico (mg/dl): Detected level: -, Normal range: 0,2 - 1\n",
    "20. Fosforo (mg/dl): Detected level: 3,90, Normal range: 2,5 - 5,6\n",
    "21. Sodio (mEq/L): Detected level: 150, Normal range: 144 - 155\n",
    "22. Potassio (mEq/L): Detected level: 4,9, Normal range: 3,3 - 5,4\n",
    "23. Rapp. Na/K: Detected level: 30,61, Normal range: > 27\n",
    "24. Cloro corr. (mEq/L): Detected level: 108,04, Normal range: 98 - 118\n",
    "25. HCO3 (mEq/L): Detected level: 21, Normal range: 17,0 - 25,2\n",
    "26. Div. anionico: Detected level: 23, Normal range: 12,0 - 24,0\n",
    "27. Osmolarità cal. (mOsm): Detected level: 322, Normal range: 314 - 335\n",
    "28. Ferro (μg/dl): Detected level: 114, Normal range: 100 - 280\n",
    "29. UBC (μg/dl): Detected level: 330, Normal range: 150 - 350\n",
    "30. TIBC (μg/dl): Detected level: 444, Normal range: 300 - 510\n",
    "31. Saturazione (%): Detected level: 25,68, Normal range: 30 - 60\n",
    "32. Ferritina (ng/ml): Detected level: -, Normal range: 21 - 78\n",
    "33. Aptoglobina HPT (mg/dl): Detected level: -, Normal range: 20 - 60\n",
    "34. Ac. Billari pre (μmol/L): Detected level: 0,3, Normal range: 0,3 - 9\n",
    "35. Ac. Billari post (μmol/L): Detected level: 0,6, Normal range: 0,6 - 30\n",
    "36. Alfa 2 Macroglobulina (g/L): Detected level: -, Normal range: 0,02 - 0,65\n",
    "37. Cistatina (mg/L): Detected level: 2282, Normal range: 3350 - 6550\n",
    "38. Colinesterasi (IU/L): Detected level: -, Normal range: 30 - 398\n",
    "39. LDH (IU/L): Detected level: -, Normal range: 188 - 351\n",
    "40. Fruttosamina (μmol/L): Detected level: -, Normal range: 0,2 - 9,0\n",
    "41. Lattato (mg/dl): Detected level: -, Normal range: 0,5 - 4,95\n",
    "42. Ceruloplasmina (mg/dl): Detected level: -, Normal range: 2,5 - 0,10\n",
    "43. Proteina C Reatt. (mg/dl): Detected level: -, Normal range: 0,01 - 0,10\n",
    "44. AGP (mg/ml): Detected level: -, Normal range: 0,01 - 0,10\n",
    "45. Magnesio (mg/dl): Detected level: 111, Normal range: 1,60 - 2,48\n",
    "46. Calcio Iónico (mmol/L): Detected level: 1,50, Normal range: 0,97 - 1,34\"\"\"\n",
    "# Combine the few-shot prompt with the user's input\n",
    "few_shot_prompt = few_shot_prompt + context + user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    full_prompt,\n",
    "    max_tokens=2048,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(full_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought prompt with an example\n",
    "cot_prompt = \"\"\"\n",
    "Analyze the following blood test results, identify abnormal values, and explain the reasoning step by step.\n",
    "\n",
    "### Example 1\n",
    "Input:\n",
    "BLOOD_TEST_DIAGNOSIS_DOCUMENT\n",
    "1. AST (IU/L): Detected level: 50, Normal range: 10 - 45\n",
    "2. ALT (IU/L): Detected level: 25, Normal range: 10 - 60\n",
    "3. ALP (IU/L): Detected level: 200, Normal range: 45 - 152\n",
    "4. Glucosio (mg/dl): Detected level: 65, Normal range: 70 - 110\n",
    "5. STI (IU/L): Detected level: -, Normal range: 10 - 100\n",
    "\n",
    "\n",
    "Let's think step by step:\n",
    "1. AST: Detected level is 50. The normal range is 10 - 45. Since 50 is higher than 45, AST is abnormal (High).\n",
    "2. ALT: Detected level is 25. The normal range is 10 - 60. Since 25 is within this range, ALT is normal.\n",
    "3. ALP: Detected level is 200. The normal range is 45 - 152. Since 200 is higher than 152, ALP is abnormal (High).\n",
    "4. Glucosio: Detected level is 65. The normal range is 70 - 110. Since 65 is lower than 70, Glucosio is abnormal (Low).\n",
    "5. STI: Detected level is -. The normal range is 10 - 100. This molecule was not tested.\n",
    "Abnormal Results:\n",
    "1. AST: High (Detected level 50, above normal range of 10 - 45).\n",
    "3. ALP: High (Detected level 200, above normal range of 45 - 152).\n",
    "4. Glucosio: Low (Detected level 65, below normal range of 70 - 110).\n",
    "\n",
    "### New Input:\n",
    "\"\"\"\n",
    "\n",
    "# Combine prompt with new input\n",
    "full_prompt = cot_prompt + user_input + context + \"\\nLet's think step by step:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    full_prompt,\n",
    "    max_tokens=4096,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(full_prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Prompt Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more detailed system prompt to guide the LLM effectively\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are analyzing blood test results. Your task is to detect abnormalities and provide a health diagnosis.\n",
    "For each abnormal result, provide the chemical name, its detected level, and indicate whether it's above or below the normal range.\n",
    "An abnormal value is defined as a value which is below the minimum normal range or above the maximum normal range.\n",
    "If no abnormal values are found, respond with \"All values are within the normal range.\"\n",
    "Do not include any irrelevant details or chemicals that are within the normal range.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Function to generate the prompt using the enhanced system and user-specific inputs\n",
    "def generate_prompt(\n",
    "    context: str, question: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Question is focused and clear\n",
    "question = \"Extract all abnormal chemical names with their detected levels and indicate if they are above or below the normal range.\"\n",
    "\n",
    "# Generate the refined prompt\n",
    "final_prompt = generate_prompt(context=context, question=question)\n",
    "\n",
    "# Print the final prompt to see how it looks\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "    final_prompt,\n",
    "    max_tokens=4096,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    echo=True,\n",
    ")  # Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"choices\"][0][\"text\"][len(final_prompt) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
