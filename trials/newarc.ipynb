{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e951c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from typing import List, Dict, Any, Optional\n",
    "from PIL import Image\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.chat_models import ChatOllama, ChatDeepInfra\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# HuggingFace imports for vision model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_community.chat_models import ChatLiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c08144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boom/.pyenv/versions/aivet/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39460672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/boom/.pyenv/versions/aivet/bin/python -m pip install --upgrade pip\n",
    "# /home/boom/.pyenv/versions/aivet/bin/python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013e7f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen2.5:7b',\n",
       " 'gemma3:12b-it-qat',\n",
       " 'gemma3:4b',\n",
       " 'qwen3:8b',\n",
       " 'qwen3:0.6b',\n",
       " 'gemma3:27b-it-qat',\n",
       " 'deepseek-r1:7b',\n",
       " 'qwen2.5:14b-instruct-q4_K_M']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama_client = ollama.Client()\n",
    "ollama_client.list()[\"models\"]\n",
    "\n",
    "available_models = [model[\"model\"] for model in ollama_client.list()[\"models\"]]\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73e391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_client.pull(f\"gemma3:12b-it-qat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from typing import List, Dict, Any, Optional\n",
    "from PIL import Image\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.chat_models import ChatOllama, ChatDeepInfra\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# HuggingFace imports for vision model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class ModelProvider:\n",
    "    \"\"\"Centralized model provider management for different LLM services.\"\"\"\n",
    "\n",
    "    def _load_ollama_model(self, model_name):\n",
    "        \"\"\"Initialize Ollama client for multimodal models.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            self.ollama_client = ollama.Client()\n",
    "\n",
    "            # Verify model exists, pull if needed\n",
    "            available_models = [\n",
    "                model[\"model\"] for model in self.ollama_client.list()[\"models\"]\n",
    "            ]\n",
    "            if model_name not in available_models:\n",
    "                print(f\"üì• Pulling Ollama model: {model_name}\")\n",
    "                self.ollama_client.pull(f\"{model_name}\")\n",
    "\n",
    "            print(f\"‚úÖ Ollama multimodal model {model_name} ready\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing Ollama model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_model(\n",
    "        self, provider: str = \"ollama\", model_name=\"qwen2.5:8b\", **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"Initialize the appropriate model based on provider.\"\"\"\n",
    "        provider = provider.lower()\n",
    "\n",
    "        print(provider)\n",
    "        print(model_name)\n",
    "        if provider == \"ollama\":\n",
    "            self._load_ollama_model(model_name)\n",
    "            if \"qwen2.5\" in model_name:\n",
    "                return ChatOllama(\n",
    "                    model=model_name,\n",
    "                    temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                    top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                    num_ctx=kwargs.get(\"num_ctx\", 32768),\n",
    "                )\n",
    "            elif \"gemma3\" in model_name:\n",
    "                return ChatOllama(\n",
    "                    model=\"gemma3:8b\",\n",
    "                    temperature=kwargs.get(\"temperature\", 1.0),\n",
    "                    top_k=kwargs.get(\"top_k\", 64),\n",
    "                    top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                    num_ctx=kwargs.get(\"num_ctx\", 32768),\n",
    "                )\n",
    "        elif provider == \"anthropic\":\n",
    "\n",
    "            return ChatLiteLLM(\n",
    "                model=\"anthropic/claude-3-5-haiku-latest\",\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "            )\n",
    "        elif provider == \"deepinfra\":\n",
    "            return ChatDeepInfra(\n",
    "                model_name=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "                deepinfra_api_token=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                top_k=kwargs.get(\"top_k\", 20),\n",
    "            )\n",
    "        elif provider == \"meta\":\n",
    "            return ChatDeepInfra(\n",
    "                model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                deepinfra_api_token=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                streaming=kwargs.get(\"streaming\", True),\n",
    "            )\n",
    "        elif provider == \"deepseek\":\n",
    "            return ChatDeepInfra(\n",
    "                model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "                api_base=\"https://api.deepinfra.com/v1/openai\",\n",
    "                api_key=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "            )\n",
    "        elif provider == \"local_llama\":\n",
    "            return LlamaCpp(\n",
    "                model_name=kwargs.get(\"model_name\", \"\"),\n",
    "                n_ctx=kwargs.get(\"n_ctx\", 5000),\n",
    "                n_threads=kwargs.get(\"n_threads\", 12),\n",
    "                n_gpu_layers=kwargs.get(\"n_gpu_layers\", 0),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"‚ùå Unsupported provider: {provider}\")\n",
    "\n",
    "\n",
    "class VeterinaryRAG:\n",
    "    \"\"\"RAG system for veterinary knowledge base.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, knowledge_base_path: str, embedding_provider: str = \"huggingface\"\n",
    "    ):\n",
    "        self.knowledge_base_path = knowledge_base_path\n",
    "        self.embedding_provider = embedding_provider\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self._setup_embeddings()\n",
    "        self._load_knowledge_base()\n",
    "\n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Initialize embedding model.\"\"\"\n",
    "        if self.embedding_provider == \"huggingface\":\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "            )\n",
    "        elif self.embedding_provider == \"ollama\":\n",
    "            self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported embedding provider: {self.embedding_provider}\"\n",
    "            )\n",
    "\n",
    "    def _load_knowledge_base(self):\n",
    "        \"\"\"Load and process veterinary knowledge base.\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        # Load documents from knowledge base path\n",
    "        if os.path.isfile(self.knowledge_base_path):\n",
    "            if self.knowledge_base_path.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(self.knowledge_base_path)\n",
    "            else:\n",
    "                loader = TextLoader(self.knowledge_base_path)\n",
    "            documents.extend(loader.load())\n",
    "        elif os.path.isdir(self.knowledge_base_path):\n",
    "            for filename in os.listdir(self.knowledge_base_path):\n",
    "                filepath = os.path.join(self.knowledge_base_path, filename)\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    loader = PyPDFLoader(filepath)\n",
    "                elif filename.endswith(\".txt\"):\n",
    "                    loader = TextLoader(filepath)\n",
    "                else:\n",
    "                    continue\n",
    "                documents.extend(loader.load())\n",
    "\n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Create vector store\n",
    "        self.vectorstore = FAISS.from_documents(splits, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": 5}\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(splits)} document chunks into vector store\")\n",
    "\n",
    "    def retrieve_context(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant context for a query.\"\"\"\n",
    "        if self.retriever is None:\n",
    "            return \"\"\n",
    "\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        return context\n",
    "\n",
    "\n",
    "class ImageToTextExtractor:\n",
    "    \"\"\"Modernized image-to-text extractor supporting both HuggingFace and Ollama multimodal models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model_name: str = None,\n",
    "        vision_model_provider: str = \"huggingface\",\n",
    "        device: str = \"auto\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.device = (\n",
    "            device\n",
    "            if device != \"auto\"\n",
    "            else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.model_name = vision_model_name\n",
    "        self.model_provider = vision_model_provider.lower()\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.ollama_client = None\n",
    "\n",
    "        # Additional kwargs for model configuration\n",
    "        self.model_config = kwargs\n",
    "\n",
    "        self._load_model()\n",
    "\n",
    "        self.extraction_query = \"\"\"Task: Extract and list all blood test details from this veterinary blood test report.\n",
    "\n",
    "        Objective:\n",
    "        1. Identify and extract **chemical/analyte names** from the blood test report.\n",
    "        2. Capture the **detected levels** of each chemical with their units.\n",
    "        3. Extract the **normal/reference range** for each corresponding chemical.\n",
    "        4. Note any flags or indicators (High, Low, Normal, etc.).\n",
    "\n",
    "        Expected Output Format:\n",
    "        - Chemical Name: [Name]\n",
    "        - Detected Level: [Value with units]\n",
    "        - Normal Range: [Min Value] - [Max Value with units]\n",
    "        - Status: [Normal/High/Low if indicated]\n",
    "\n",
    "        Please ensure accurate extraction, including any unit symbols (e.g., mg/dL, IU/L, g/L), and handle variations in formatting or alignment. Return results in a structured list format for easy readability.\"\"\"\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the appropriate vision model based on provider.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            self._load_huggingface_model()\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            self._load_ollama_model()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model provider: {self.model_provider}\")\n",
    "\n",
    "    def _load_huggingface_model(self):\n",
    "        \"\"\"Load HuggingFace vision model.\"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"auto\",\n",
    "                use_cache=True,\n",
    "                **self.model_config,\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, trust_remote_code=True\n",
    "            )\n",
    "            print(f\"‚úÖ HuggingFace vision model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading HuggingFace vision model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _load_ollama_model(self):\n",
    "        \"\"\"Initialize Ollama client for multimodal models.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            self.ollama_client = ollama.Client()\n",
    "\n",
    "            # Verify model exists, pull if needed\n",
    "            available_models = [\n",
    "                model[\"model\"] for model in self.ollama_client.list()[\"models\"]\n",
    "            ]\n",
    "            if self.model_name not in available_models:\n",
    "                print(f\"üì• Pulling Ollama model: {self.model_name}\")\n",
    "                self.ollama_client.pull(f\"{self.model_name}\")\n",
    "\n",
    "            print(f\"‚úÖ Ollama multimodal model {self.model_name} ready\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Ollama package not found. Install with: pip install ollama\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing Ollama model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU memory based on provider.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                del self.tokenizer\n",
    "                self.model = None\n",
    "                self.tokenizer = None\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            # Ollama manages its own memory, but we can clear the client reference\n",
    "            self.ollama_client = None\n",
    "        print(\"üßπ Memory cleared\")\n",
    "\n",
    "    def convert_pdf_to_images(\n",
    "        self, pdf_path: str, zoom: float = 2.0\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"Convert PDF pages to PIL Images.\"\"\"\n",
    "        images = []\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                img_data = pix.tobytes(\"png\")\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                images.append(img.convert(\"RGB\"))\n",
    "                print(f\"üìÑ Converted page {page_num + 1} to image\")\n",
    "            doc.close()\n",
    "            return images\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error converting PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF or image file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        try:\n",
    "            if file_extension == \".pdf\":\n",
    "                print(f\"üìã Processing PDF file: {file_path}\")\n",
    "                images = self.convert_pdf_to_images(file_path)\n",
    "                for i, image in enumerate(images):\n",
    "                    print(f\"üîç Extracting text from page {i + 1}...\")\n",
    "                    page_text = self._process_image(image, self.extraction_query)\n",
    "                    extracted_text += f\"=== Page {i + 1} ===\\n{page_text}\\n\\n\"\n",
    "            elif file_extension in [\".png\", \".jpeg\", \".jpg\", \".bmp\"]:\n",
    "                print(f\"üñºÔ∏è Processing image file: {file_path}\")\n",
    "                image = Image.open(file_path).convert(\"RGB\")\n",
    "                extracted_text = self._process_image(image, self.extraction_query)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "            return extracted_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during text extraction: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _process_image(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process a single image with the appropriate vision model.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            return self._process_image_huggingface(image, query)\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            return self._process_image_ollama(image, query)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model provider: {self.model_provider}\")\n",
    "\n",
    "    def _process_image_huggingface(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process image using HuggingFace model.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\n",
    "                \"HuggingFace model not loaded. Call _load_model() first.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"image\": image, \"content\": query}],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 2500,\n",
    "                \"do_sample\": True,\n",
    "                \"top_k\": 1,\n",
    "                \"temperature\": 0.2,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "                outputs = outputs[:, inputs[\"input_ids\"].shape[1] :]\n",
    "                output_text = self.tokenizer.decode(\n",
    "                    outputs[0], skip_special_tokens=True\n",
    "                )\n",
    "                return output_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image with HuggingFace: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _process_image_ollama(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process image using Ollama multimodal model.\"\"\"\n",
    "        if self.ollama_client is None:\n",
    "            raise RuntimeError(\n",
    "                \"Ollama client not initialized. Call _load_model() first.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            import base64\n",
    "            from io import BytesIO\n",
    "\n",
    "            # Convert PIL image to base64\n",
    "            buffer = BytesIO()\n",
    "            image.save(buffer, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "            # Make request to Ollama\n",
    "            response = self.ollama_client.generate(\n",
    "                model=self.model_name,\n",
    "                prompt=query,\n",
    "                images=[img_base64],\n",
    "                options={\n",
    "                    \"temperature\": self.model_config.get(\"temperature\", 0.2),\n",
    "                    \"top_k\": self.model_config.get(\"top_k\", 1),\n",
    "                    \"top_p\": self.model_config.get(\"top_p\", 0.9),\n",
    "                    \"num_ctx\": self.model_config.get(\n",
    "                        \"num_ctx\", 128000\n",
    "                    ),  # Gemma 3 supports 128K context\n",
    "                },\n",
    "            )\n",
    "\n",
    "            return response[\"response\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image with Ollama: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "class VeterinaryTextAnalyzer:\n",
    "    \"\"\"Modernized text analyzer with LangChain integration and RAG support.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:7b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "        print(\"1\", llm_provider)\n",
    "        self.llm_provider = llm_provider\n",
    "        provider_instance = ModelProvider()\n",
    "\n",
    "        self.llm = provider_instance.initialize_model(\n",
    "            provider=llm_provider, model_name=llm_model_name, **llm_kwargs\n",
    "        )\n",
    "        self.rag_system = None\n",
    "\n",
    "        if knowledge_base_path and os.path.exists(knowledge_base_path):\n",
    "            self.rag_system = VeterinaryRAG(knowledge_base_path)\n",
    "            print(\"‚úÖ RAG system initialized\")\n",
    "\n",
    "        self._setup_prompts()\n",
    "        self._setup_chains()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Setup prompt templates for different analysis modes.\"\"\"\n",
    "\n",
    "        # Chain of Thought prompt\n",
    "        self.cot_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        You are a veterinary assistant analyzing blood test results for a dog. Follow these steps:\n",
    "\n",
    "        Step 1: Review the blood test results below.\n",
    "        Step 2: For each molecule, compare detected level with normal range and determine if abnormal.\n",
    "        Step 3: For abnormal values, explain the clinical significance.\n",
    "        Step 4: Provide a comprehensive health assessment.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Blood test results:\n",
    "        {blood_results}\n",
    "\n",
    "        Please provide your detailed step-by-step analysis:\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Few-shot prompt\n",
    "        self.few_shot_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        You are a veterinary assistant analyzing blood test results. Identify abnormal values and their implications.\n",
    "\n",
    "        Example Analysis:\n",
    "        Blood results: AST: 50 (Normal: 10-45), ALT: 25 (Normal: 10-60), ALP: 200 (Normal: 45-152)\n",
    "        \n",
    "        Analysis:\n",
    "        - AST: High (50, normal 10-45) - possible liver/muscle damage\n",
    "        - ALP: High (200, normal 45-152) - possible liver disease or bone disorders\n",
    "        \n",
    "        Summary: Elevated liver enzymes suggest hepatic dysfunction requiring further investigation.\n",
    "\n",
    "        ---\n",
    "\n",
    "        Veterinary Context:\n",
    "        {context}\n",
    "\n",
    "        Now analyze these results:\n",
    "        {blood_results}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Summary prompt\n",
    "        self.summary_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        Based on the veterinary knowledge provided and the blood test analysis, provide a concise health summary.\n",
    "\n",
    "        Veterinary Knowledge:\n",
    "        {context}\n",
    "\n",
    "        Blood Test Analysis:\n",
    "        {analysis_results}\n",
    "\n",
    "        Provide a focused summary covering:\n",
    "        1. Key abnormalities and their clinical significance\n",
    "        2. Potential diagnoses or conditions\n",
    "        3. Recommended next steps (if warranted)\n",
    "\n",
    "        Keep response to 5 sentences maximum.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    def _setup_chains(self):\n",
    "        \"\"\"Setup LangChain processing chains.\"\"\"\n",
    "        self.cot_chain = self.cot_prompt | self.llm | StrOutputParser()\n",
    "        self.few_shot_chain = self.few_shot_prompt | self.llm | StrOutputParser()\n",
    "        self.summary_chain = self.summary_prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def _get_context(self, query: str) -> str:\n",
    "        \"\"\"Get relevant context from RAG system if available.\"\"\"\n",
    "        if self.rag_system:\n",
    "            return self.rag_system.retrieve_context(query)\n",
    "        return \"No additional veterinary knowledge available.\"\n",
    "\n",
    "    def analyze_blood_results(self, blood_results: str, mode: str = \"few_shot\") -> str:\n",
    "        \"\"\"Analyze blood test results using specified mode.\"\"\"\n",
    "        context = self._get_context(f\"blood test analysis {blood_results}\")\n",
    "\n",
    "        try:\n",
    "            if mode == \"chain_of_thought\":\n",
    "                result = self.cot_chain.invoke(\n",
    "                    {\"context\": context, \"blood_results\": blood_results}\n",
    "                )\n",
    "            elif mode == \"few_shot\":\n",
    "                result = self.few_shot_chain.invoke(\n",
    "                    {\"context\": context, \"blood_results\": blood_results}\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported analysis mode: {mode}\")\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during analysis: {e}\")\n",
    "            return f\"Analysis failed: {str(e)}\"\n",
    "\n",
    "    def generate_summary(self, analysis_results: str) -> str:\n",
    "        \"\"\"Generate a summary of the analysis results.\"\"\"\n",
    "        context = self._get_context(f\"veterinary diagnosis summary {analysis_results}\")\n",
    "\n",
    "        try:\n",
    "            summary = self.summary_chain.invoke(\n",
    "                {\"context\": context, \"analysis_results\": analysis_results}\n",
    "            )\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating summary: {e}\")\n",
    "            return f\"Summary generation failed: {str(e)}\"\n",
    "\n",
    "\n",
    "class AiVetPipeline:\n",
    "    \"\"\"Complete pipeline for veterinary blood test analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model_provider: str = \"ollama\",\n",
    "        vision_model_name: str = \"gemma3:4b\",\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:7b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "\n",
    "        self.extractor = ImageToTextExtractor(\n",
    "            vision_model_name=vision_model_name,\n",
    "            vision_model_provider=vision_model_provider,\n",
    "        )\n",
    "        self.analyzer = VeterinaryTextAnalyzer(\n",
    "            llm_provider, llm_model_name, knowledge_base_path, **llm_kwargs\n",
    "        )\n",
    "        print(\"üè• AiVet Pipeline initialized successfully\")\n",
    "\n",
    "    def process_blood_test(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        analysis_mode: str = \"few_shot\",\n",
    "        generate_summary: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Complete processing pipeline for blood test analysis.\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        try:\n",
    "            # Step 1: Extract text from image/PDF\n",
    "            print(\"üîç Step 1: Extracting text from image...\")\n",
    "            extracted_text = self.extractor.extract_text_from_file(file_path)\n",
    "            results[\"extracted_text\"] = extracted_text\n",
    "\n",
    "            # Step 2: Analyze blood results\n",
    "            print(\"üß™ Step 2: Analyzing blood test results...\")\n",
    "            analysis = self.analyzer.analyze_blood_results(\n",
    "                extracted_text, analysis_mode\n",
    "            )\n",
    "            results[\"analysis\"] = analysis\n",
    "\n",
    "            # Step 3: Generate summary (optional)\n",
    "            if generate_summary:\n",
    "                print(\"üìã Step 3: Generating health summary...\")\n",
    "                summary = self.analyzer.generate_summary(analysis)\n",
    "                results[\"summary\"] = summary\n",
    "\n",
    "            print(\"‚úÖ Pipeline completed successfully\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pipeline failed: {e}\")\n",
    "            results[\"error\"] = str(e)\n",
    "            return results\n",
    "        finally:\n",
    "            # Clean up GPU memory\n",
    "            self.extractor.clear_memory()\n",
    "\n",
    "\n",
    "class AiVetPipeline:\n",
    "    \"\"\"Complete pipeline for veterinary blood test analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_provider: str = \"ollama\",\n",
    "        vision_model_name: str = \"gemma3:4b\",\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:8b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        vision_config: Optional[Dict] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "\n",
    "        vision_config = vision_config or {}\n",
    "        self.extractor = ImageToTextExtractor(\n",
    "            vision_model_name=vision_model_name,\n",
    "            vision_model_provider=vision_provider,\n",
    "            **vision_config,\n",
    "        )\n",
    "        self.analyzer = VeterinaryTextAnalyzer(\n",
    "            llm_provider, llm_model_name, knowledge_base_path, **llm_kwargs\n",
    "        )\n",
    "        print(\n",
    "            f\"üè• AiVet Pipeline initialized with {vision_provider} vision and {llm_provider} LLM\"\n",
    "        )\n",
    "\n",
    "    def process_blood_test(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        analysis_mode: str = \"few_shot\",\n",
    "        generate_summary: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Complete processing pipeline for blood test analysis.\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        try:\n",
    "            # Step 1: Extract text from image/PDF\n",
    "            print(\"üîç Step 1: Extracting text from image...\")\n",
    "            extracted_text = self.extractor.extract_text_from_file(file_path)\n",
    "            results[\"extracted_text\"] = extracted_text\n",
    "\n",
    "            # Step 2: Analyze blood results\n",
    "            print(\"üß™ Step 2: Analyzing blood test results...\")\n",
    "            analysis = self.analyzer.analyze_blood_results(\n",
    "                extracted_text, analysis_mode\n",
    "            )\n",
    "            results[\"analysis\"] = analysis\n",
    "\n",
    "            # Step 3: Generate summary (optional)\n",
    "            if generate_summary:\n",
    "                print(\"üìã Step 3: Generating health summary...\")\n",
    "                summary = self.analyzer.generate_summary(analysis)\n",
    "                results[\"summary\"] = summary\n",
    "\n",
    "            print(\"‚úÖ Pipeline completed successfully\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pipeline failed: {e}\")\n",
    "            results[\"error\"] = str(e)\n",
    "            return results\n",
    "        finally:\n",
    "            # Clean up GPU memory\n",
    "            self.extractor.clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9e8201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Pulling Ollama model: llama3.2-vision:11b\n",
      "‚úÖ Ollama multimodal model llama3.2-vision:11b ready\n",
      "1 ollama\n",
      "ollama\n",
      "qwen2.5:7b\n",
      "‚úÖ Ollama multimodal model qwen2.5:7b ready\n",
      "üè• AiVet Pipeline initialized with ollama vision and ollama LLM\n",
      "üîç Step 1: Extracting text from image...\n",
      "üìã Processing PDF file: ../data/input/202402_hope_emo.pdf\n",
      "üìÑ Converted page 1 to image\n",
      "üîç Extracting text from page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26807/1249880370.py:59: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      9\u001b[39m pipeline_ollama = AiVetPipeline(\n\u001b[32m     10\u001b[39m     vision_model_name=\u001b[33m\"\u001b[39m\u001b[33mllama3.2-vision:11b\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# or \"gemma2:27b-vision\" for larger model\u001b[39;00m\n\u001b[32m     11\u001b[39m     vision_provider=\u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     },\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Process a blood test image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m results = \u001b[43mpipeline_ollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_blood_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/input/202402_hope_emo.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43manalysis_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfew_shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerate_summary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m results.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 681\u001b[39m, in \u001b[36mAiVetPipeline.process_blood_test\u001b[39m\u001b[34m(self, file_path, analysis_mode, generate_summary)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    679\u001b[39m     \u001b[38;5;66;03m# Step 1: Extract text from image/PDF\u001b[39;00m\n\u001b[32m    680\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Step 1: Extracting text from image...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     extracted_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m     results[\u001b[33m\"\u001b[39m\u001b[33mextracted_text\u001b[39m\u001b[33m\"\u001b[39m] = extracted_text\n\u001b[32m    684\u001b[39m     \u001b[38;5;66;03m# Step 2: Analyze blood results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 341\u001b[39m, in \u001b[36mImageToTextExtractor.extract_text_from_file\u001b[39m\u001b[34m(self, file_path)\u001b[39m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[32m    340\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Extracting text from page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m         page_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextraction_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m         extracted_text += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=== Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpage_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.bmp\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 360\u001b[39m, in \u001b[36mImageToTextExtractor._process_image\u001b[39m\u001b[34m(self, image, query)\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_image_huggingface(image, query)\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_provider == \u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_image_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported model provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 419\u001b[39m, in \u001b[36mImageToTextExtractor._process_image_ollama\u001b[39m\u001b[34m(self, image, query)\u001b[39m\n\u001b[32m    416\u001b[39m     img_base64 = base64.b64encode(buffer.getvalue()).decode()\n\u001b[32m    418\u001b[39m     \u001b[38;5;66;03m# Make request to Ollama\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mollama_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_base64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_ctx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_ctx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128000\u001b[39;49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gemma 3 supports 128K context\u001b[39;49;00m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/ollama/_client.py:247\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    221\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    222\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    235\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    236\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    237\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/ollama/_client.py:120\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    119\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     r.raise_for_status()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# pipeline_hf = AiVetPipeline(\n",
    "#     vision_model_name=\"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     vision_provider=\"huggingface\",\n",
    "#     llm_provider=\"qwen\",\n",
    "#     knowledge_base_path=\"./vet_knowledge_base/\"\n",
    "# )\n",
    "\n",
    "# Example 2: Using Ollama Gemma 3 multimodal model\n",
    "pipeline_ollama = AiVetPipeline(\n",
    "    vision_model_name=\"llama3.2-vision:11b\",  # or \"gemma2:27b-vision\" for larger model\n",
    "    vision_provider=\"ollama\",\n",
    "    llm_provider=\"ollama\",  # Using Ollama for text analysis too\n",
    "    llm_model_name=\"qwen2.5:7b\",  # Using Ollama for text analysis too\n",
    "    knowledge_base_path=\"./vet_knowledge_base/\",\n",
    "    vision_config={\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_k\": 1,\n",
    "        \"num_ctx\": 128000,  # Take advantage of Gemma 3's 128K context\n",
    "    },\n",
    ")\n",
    "\n",
    "# Process a blood test image\n",
    "results = pipeline_ollama.process_blood_test(\n",
    "    file_path=\"../data/input/202402_hope_emo.pdf\",\n",
    "    analysis_mode=\"few_shot\",\n",
    "    generate_summary=True,\n",
    ")\n",
    "\n",
    "print(\"üìä Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"\\n{key.upper()}:\\n{value}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64748a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
