{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c08144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boom/.pyenv/versions/aivet/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958cedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boom/.pyenv/versions/aivet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from typing import List, Dict, Any, Optional\n",
    "from PIL import Image\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.chat_models import ChatDeepInfra\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# HuggingFace imports for vision model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39460672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/boom/.pyenv/versions/aivet/bin/python -m pip install --upgrade pip\n",
    "# /home/boom/.pyenv/versions/aivet/bin/python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013e7f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen2.5vl:7b',\n",
       " 'llama3.2-vision:11b',\n",
       " 'gemma3:12b-it-qat',\n",
       " 'qwen2.5:7b',\n",
       " 'gemma3:4b',\n",
       " 'qwen3:8b',\n",
       " 'qwen3:0.6b',\n",
       " 'gemma3:27b-it-qat',\n",
       " 'deepseek-r1:7b',\n",
       " 'qwen2.5:14b-instruct-q4_K_M']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama_client = ollama.Client()\n",
    "ollama_client.list()[\"models\"]\n",
    "\n",
    "available_models = [model[\"model\"] for model in ollama_client.list()[\"models\"]]\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73e391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_client.pull(f\"gemma3:12b-it-qat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider:\n",
    "    \"\"\"Centralized model provider management for different LLM services.\"\"\"\n",
    "\n",
    "    def _load_ollama_model(self, model_name):\n",
    "        \"\"\"Initialize Ollama client for multimodal models.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            self.ollama_client = ollama.Client()\n",
    "\n",
    "            # Verify model exists, pull if needed\n",
    "            available_models = [\n",
    "                model[\"model\"] for model in self.ollama_client.list()[\"models\"]\n",
    "            ]\n",
    "            if model_name not in available_models:\n",
    "                print(f\"üì• Pulling Ollama model: {model_name}\")\n",
    "                self.ollama_client.pull(f\"{model_name}\")\n",
    "\n",
    "            print(f\"‚úÖ Ollama multimodal model {model_name} ready\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing Ollama model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_model(\n",
    "        self, provider: str = \"ollama\", model_name=\"qwen2.5:8b\", **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"Initialize the appropriate model based on provider.\"\"\"\n",
    "        provider = provider.lower()\n",
    "\n",
    "        print(provider)\n",
    "        print(model_name)\n",
    "        if provider == \"ollama\":\n",
    "            self._load_ollama_model(model_name)\n",
    "            if \"qwen2.5\" in model_name:\n",
    "                return ChatOllama(\n",
    "                    model=model_name,\n",
    "                    temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                    top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                    num_ctx=kwargs.get(\"num_ctx\", 32768),\n",
    "                )\n",
    "            elif \"gemma3\" in model_name:\n",
    "                return ChatOllama(\n",
    "                    model=\"gemma3:8b\",\n",
    "                    temperature=kwargs.get(\"temperature\", 1.0),\n",
    "                    top_k=kwargs.get(\"top_k\", 64),\n",
    "                    top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                    num_ctx=kwargs.get(\"num_ctx\", 32768),\n",
    "                )\n",
    "        elif provider == \"anthropic\":\n",
    "\n",
    "            return ChatLiteLLM(\n",
    "                model=\"anthropic/claude-3-5-haiku-latest\",\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "            )\n",
    "        elif provider == \"deepinfra\":\n",
    "            return ChatDeepInfra(\n",
    "                model_name=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "                deepinfra_api_token=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                top_k=kwargs.get(\"top_k\", 20),\n",
    "            )\n",
    "        elif provider == \"meta\":\n",
    "            return ChatDeepInfra(\n",
    "                model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                deepinfra_api_token=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 8192),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                streaming=kwargs.get(\"streaming\", True),\n",
    "            )\n",
    "        elif provider == \"deepseek\":\n",
    "            return ChatDeepInfra(\n",
    "                model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "                api_base=\"https://api.deepinfra.com/v1/openai\",\n",
    "                api_key=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.6),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "            )\n",
    "        elif provider == \"local_llama\":\n",
    "            return LlamaCpp(\n",
    "                model_name=kwargs.get(\"model_name\", \"\"),\n",
    "                n_ctx=kwargs.get(\"n_ctx\", 5000),\n",
    "                n_threads=kwargs.get(\"n_threads\", 12),\n",
    "                n_gpu_layers=kwargs.get(\"n_gpu_layers\", 0),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                top_p=kwargs.get(\"top_p\", 0.95),\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"‚ùå Unsupported provider: {provider}\")\n",
    "\n",
    "\n",
    "class ImageToTextExtractor:\n",
    "    \"\"\"Modernized image-to-text extractor supporting both HuggingFace and Ollama multimodal models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model_name: str = None,\n",
    "        vision_model_provider: str = \"huggingface\",\n",
    "        device: str = \"auto\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.device = (\n",
    "            device\n",
    "            if device != \"auto\"\n",
    "            else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.model_name = vision_model_name\n",
    "        self.model_provider = vision_model_provider.lower()\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.ollama_client = None\n",
    "\n",
    "        # Additional kwargs for model configuration\n",
    "        self.model_config = kwargs\n",
    "\n",
    "        self._load_model()\n",
    "\n",
    "        self.extraction_query = \"\"\"Task: Extract and list all blood test details from this veterinary blood test report.\n",
    "\n",
    "        Objective:\n",
    "        1. Identify and extract **chemical/analyte names** from the blood test report.\n",
    "        2. Capture the **detected levels** of each chemical with their units.\n",
    "        3. Extract the **normal/reference range** for each corresponding chemical.\n",
    "        4. Note any flags or indicators (High, Low, Normal, etc.).\n",
    "\n",
    "        Expected Output Format:\n",
    "        - Chemical Name: [Name]\n",
    "        - Detected Level: [Value with units]\n",
    "        - Normal Range: [Min Value] - [Max Value with units]\n",
    "        - Status: [Normal/High/Low if indicated]\n",
    "\n",
    "        Please ensure accurate extraction, including any unit symbols (e.g., mg/dL, IU/L, g/L), and handle variations in formatting or alignment. Return results in a structured list format for easy readability.\"\"\"\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the appropriate vision model based on provider.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            self._load_huggingface_model()\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            self._load_ollama_model()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model provider: {self.model_provider}\")\n",
    "\n",
    "    def _load_huggingface_model(self):\n",
    "        \"\"\"Load HuggingFace vision model.\"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"auto\",\n",
    "                use_cache=True,\n",
    "                **self.model_config,\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, trust_remote_code=True\n",
    "            )\n",
    "            print(f\"‚úÖ HuggingFace vision model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading HuggingFace vision model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _load_ollama_model(self):\n",
    "        \"\"\"Initialize Ollama client for multimodal models.\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            self.ollama_client = ollama.Client()\n",
    "\n",
    "            # Verify model exists, pull if needed\n",
    "            available_models = [\n",
    "                model[\"model\"] for model in self.ollama_client.list()[\"models\"]\n",
    "            ]\n",
    "            if self.model_name not in available_models:\n",
    "                print(f\"üì• Pulling Ollama model: {self.model_name}\")\n",
    "                self.ollama_client.pull(f\"{self.model_name}\")\n",
    "\n",
    "            print(f\"‚úÖ Ollama multimodal model {self.model_name} ready\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Ollama package not found. Install with: pip install ollama\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing Ollama model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU memory based on provider.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                del self.tokenizer\n",
    "                self.model = None\n",
    "                self.tokenizer = None\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            # Ollama manages its own memory, but we can clear the client reference\n",
    "            self.ollama_client = None\n",
    "        print(\"üßπ Memory cleared\")\n",
    "\n",
    "    def convert_pdf_to_images(\n",
    "        self, pdf_path: str, zoom: float = 2.0\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"Convert PDF pages to PIL Images.\"\"\"\n",
    "        images = []\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                img_data = pix.tobytes(\"png\")\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                images.append(img.convert(\"RGB\"))\n",
    "                print(f\"üìÑ Converted page {page_num + 1} to image\")\n",
    "            doc.close()\n",
    "            return images\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error converting PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF or image file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        extracted_text = \"\"\n",
    "\n",
    "        try:\n",
    "            if file_extension == \".pdf\":\n",
    "                print(f\"üìã Processing PDF file: {file_path}\")\n",
    "                images = self.convert_pdf_to_images(file_path)\n",
    "                for i, image in enumerate(images):\n",
    "                    print(f\"üîç Extracting text from page {i + 1}...\")\n",
    "                    page_text = self._process_image(image, self.extraction_query)\n",
    "                    extracted_text += f\"=== Page {i + 1} ===\\n{page_text}\\n\\n\"\n",
    "            elif file_extension in [\".png\", \".jpeg\", \".jpg\", \".bmp\"]:\n",
    "                print(f\"üñºÔ∏è Processing image file: {file_path}\")\n",
    "                image = Image.open(file_path).convert(\"RGB\")\n",
    "                extracted_text = self._process_image(image, self.extraction_query)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "            return extracted_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during text extraction: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _process_image(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process a single image with the appropriate vision model.\"\"\"\n",
    "        if self.model_provider == \"huggingface\":\n",
    "            return self._process_image_huggingface(image, query)\n",
    "        elif self.model_provider == \"ollama\":\n",
    "            return self._process_image_ollama(image, query)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model provider: {self.model_provider}\")\n",
    "\n",
    "    def _process_image_huggingface(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process image using HuggingFace model.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\n",
    "                \"HuggingFace model not loaded. Call _load_model() first.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"image\": image, \"content\": query}],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 2500,\n",
    "                \"do_sample\": True,\n",
    "                \"top_k\": 1,\n",
    "                \"temperature\": 0.2,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "                outputs = outputs[:, inputs[\"input_ids\"].shape[1] :]\n",
    "                output_text = self.tokenizer.decode(\n",
    "                    outputs[0], skip_special_tokens=True\n",
    "                )\n",
    "                return output_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image with HuggingFace: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _process_image_ollama(self, image: Image.Image, query: str) -> str:\n",
    "        \"\"\"Process image using Ollama multimodal model.\"\"\"\n",
    "        if self.ollama_client is None:\n",
    "            raise RuntimeError(\n",
    "                \"Ollama client not initialized. Call _load_model() first.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            import base64\n",
    "            from io import BytesIO\n",
    "\n",
    "            # Convert PIL image to base64\n",
    "            buffer = BytesIO()\n",
    "            image.save(buffer, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "            # Make request to Ollama\n",
    "            response = self.ollama_client.generate(\n",
    "                model=self.model_name,\n",
    "                prompt=query,\n",
    "                images=[img_base64],\n",
    "                options={\n",
    "                    \"temperature\": self.model_config.get(\"temperature\", 0.2),\n",
    "                    \"top_k\": self.model_config.get(\"top_k\", 1),\n",
    "                    \"top_p\": self.model_config.get(\"top_p\", 0.9),\n",
    "                    \"num_ctx\": self.model_config.get(\n",
    "                        \"num_ctx\", 128000\n",
    "                    ),  # Gemma 3 supports 128K context\n",
    "                },\n",
    "            )\n",
    "\n",
    "            return response[\"response\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image with Ollama: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import torch\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class VeterinaryRAG:\n",
    "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.vectorstore = Chroma(\n",
    "            embedding=self._setup_embeddings(), persist_directory=self.persist_directory\n",
    "        )\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": 5}\n",
    "        )\n",
    "        print(\"üìö VeterinaryRAG loaded vector store\")\n",
    "\n",
    "    def _setup_embeddings(self):\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "        )\n",
    "\n",
    "\n",
    "class VeterinaryTextAnalyzer:\n",
    "    \"\"\"Modernized text analyzer with LangChain integration and RAG support.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:7b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "        print(\"1\", llm_provider)\n",
    "        self.llm_provider = llm_provider\n",
    "        provider_instance = ModelProvider()\n",
    "\n",
    "        self.llm = provider_instance.initialize_model(\n",
    "            provider=llm_provider, model_name=llm_model_name, **llm_kwargs\n",
    "        )\n",
    "        self.rag_system = None\n",
    "\n",
    "        if knowledge_base_path and os.path.exists(knowledge_base_path):\n",
    "            self.rag_system = VeterinaryRAG(knowledge_base_path)\n",
    "            print(\"‚úÖ RAG system initialized\")\n",
    "\n",
    "        self._setup_prompts()\n",
    "        self._setup_chains()\n",
    "\n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Setup prompt templates for different analysis modes.\"\"\"\n",
    "\n",
    "        # Chain of Thought prompt\n",
    "        self.cot_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        You are a veterinary assistant analyzing blood test results for a dog. Follow these steps:\n",
    "\n",
    "        Step 1: Review the blood test results below.\n",
    "        Step 2: For each molecule, compare detected level with normal range and determine if abnormal.\n",
    "        Step 3: For abnormal values, explain the clinical significance.\n",
    "        Step 4: Provide a comprehensive health assessment.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Blood test results:\n",
    "        {blood_results}\n",
    "\n",
    "        Please provide your detailed step-by-step analysis:\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Few-shot prompt\n",
    "        self.few_shot_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        You are a veterinary assistant analyzing blood test results. Identify abnormal values and their implications.\n",
    "\n",
    "        Example Analysis:\n",
    "        Blood results: AST: 50 (Normal: 10-45), ALT: 25 (Normal: 10-60), ALP: 200 (Normal: 45-152)\n",
    "        \n",
    "        Analysis:\n",
    "        - AST: High (50, normal 10-45) - possible liver/muscle damage\n",
    "        - ALP: High (200, normal 45-152) - possible liver disease or bone disorders\n",
    "        \n",
    "        Summary: Elevated liver enzymes suggest hepatic dysfunction requiring further investigation.\n",
    "\n",
    "        ---\n",
    "\n",
    "        Veterinary Context:\n",
    "        {context}\n",
    "\n",
    "        Now analyze these results:\n",
    "        {blood_results}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # Summary prompt\n",
    "        self.summary_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        Based on the veterinary knowledge provided and the blood test analysis, provide a concise health summary.\n",
    "\n",
    "        Veterinary Knowledge:\n",
    "        {context}\n",
    "\n",
    "        Blood Test Analysis:\n",
    "        {analysis_results}\n",
    "\n",
    "        Provide a focused summary covering:\n",
    "        1. Key abnormalities and their clinical significance\n",
    "        2. Potential diagnoses or conditions\n",
    "        3. Recommended next steps (if warranted)\n",
    "\n",
    "        Keep response to 5 sentences maximum.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    def _setup_chains(self):\n",
    "        \"\"\"Setup LangChain processing chains.\"\"\"\n",
    "        self.cot_chain = self.cot_prompt | self.llm | StrOutputParser()\n",
    "        self.few_shot_chain = self.few_shot_prompt | self.llm | StrOutputParser()\n",
    "        self.summary_chain = self.summary_prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def _get_context(self, query: str) -> str:\n",
    "        \"\"\"Get relevant context from RAG system if available.\"\"\"\n",
    "        if self.rag_system:\n",
    "            return self.rag_system.retrieve_context(query)\n",
    "        return \"No additional veterinary knowledge available.\"\n",
    "\n",
    "    def analyze_blood_results(self, blood_results: str, mode: str = \"few_shot\") -> str:\n",
    "        \"\"\"Analyze blood test results using specified mode.\"\"\"\n",
    "        context = self._get_context(f\"blood test analysis {blood_results}\")\n",
    "\n",
    "        try:\n",
    "            if mode == \"chain_of_thought\":\n",
    "                result = self.cot_chain.invoke(\n",
    "                    {\"context\": context, \"blood_results\": blood_results}\n",
    "                )\n",
    "            elif mode == \"few_shot\":\n",
    "                result = self.few_shot_chain.invoke(\n",
    "                    {\"context\": context, \"blood_results\": blood_results}\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported analysis mode: {mode}\")\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during analysis: {e}\")\n",
    "            return f\"Analysis failed: {str(e)}\"\n",
    "\n",
    "    def generate_summary(self, analysis_results: str) -> str:\n",
    "        \"\"\"Generate a summary of the analysis results.\"\"\"\n",
    "        context = self._get_context(f\"veterinary diagnosis summary {analysis_results}\")\n",
    "\n",
    "        try:\n",
    "            summary = self.summary_chain.invoke(\n",
    "                {\"context\": context, \"analysis_results\": analysis_results}\n",
    "            )\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating summary: {e}\")\n",
    "            return f\"Summary generation failed: {str(e)}\"\n",
    "\n",
    "\n",
    "class AiVetPipeline:\n",
    "    \"\"\"Complete pipeline for veterinary blood test analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model_provider: str = \"ollama\",\n",
    "        vision_model_name: str = \"gemma3:4b\",\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:7b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "\n",
    "        self.extractor = ImageToTextExtractor(\n",
    "            vision_model_name=vision_model_name,\n",
    "            vision_model_provider=vision_model_provider,\n",
    "        )\n",
    "        self.analyzer = VeterinaryTextAnalyzer(\n",
    "            llm_provider, llm_model_name, knowledge_base_path, **llm_kwargs\n",
    "        )\n",
    "        print(\"üè• AiVet Pipeline initialized successfully\")\n",
    "\n",
    "    def process_blood_test(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        analysis_mode: str = \"few_shot\",\n",
    "        generate_summary: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Complete processing pipeline for blood test analysis.\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        try:\n",
    "            # Step 1: Extract text from image/PDF\n",
    "            print(\"üîç Step 1: Extracting text from image...\")\n",
    "            extracted_text = self.extractor.extract_text_from_file(file_path)\n",
    "            results[\"extracted_text\"] = extracted_text\n",
    "\n",
    "            # Step 2: Analyze blood results\n",
    "            print(\"üß™ Step 2: Analyzing blood test results...\")\n",
    "            analysis = self.analyzer.analyze_blood_results(\n",
    "                extracted_text, analysis_mode\n",
    "            )\n",
    "            results[\"analysis\"] = analysis\n",
    "\n",
    "            # Step 3: Generate summary (optional)\n",
    "            if generate_summary:\n",
    "                print(\"üìã Step 3: Generating health summary...\")\n",
    "                summary = self.analyzer.generate_summary(analysis)\n",
    "                results[\"summary\"] = summary\n",
    "\n",
    "            print(\"‚úÖ Pipeline completed successfully\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pipeline failed: {e}\")\n",
    "            results[\"error\"] = str(e)\n",
    "            return results\n",
    "        finally:\n",
    "            # Clean up GPU memory\n",
    "            self.extractor.clear_memory()\n",
    "\n",
    "\n",
    "class AiVetPipeline:\n",
    "    \"\"\"Complete pipeline for veterinary blood test analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_provider: str = \"ollama\",\n",
    "        vision_model_name: str = \"gemma3:4b\",\n",
    "        llm_provider: str = \"ollama\",\n",
    "        llm_model_name: str = \"qwen2.5:8b\",\n",
    "        knowledge_base_path: Optional[str] = None,\n",
    "        vision_config: Optional[Dict] = None,\n",
    "        **llm_kwargs,\n",
    "    ):\n",
    "\n",
    "        vision_config = vision_config or {}\n",
    "        self.extractor = ImageToTextExtractor(\n",
    "            vision_model_name=vision_model_name,\n",
    "            vision_model_provider=vision_provider,\n",
    "            **vision_config,\n",
    "        )\n",
    "        self.analyzer = VeterinaryTextAnalyzer(\n",
    "            llm_provider, llm_model_name, knowledge_base_path, **llm_kwargs\n",
    "        )\n",
    "        print(\n",
    "            f\"üè• AiVet Pipeline initialized with {vision_provider} vision and {llm_provider} LLM\"\n",
    "        )\n",
    "\n",
    "    def process_blood_test(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        analysis_mode: str = \"few_shot\",\n",
    "        generate_summary: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Complete processing pipeline for blood test analysis.\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        try:\n",
    "            # Step 1: Extract text from image/PDF\n",
    "            print(\"üîç Step 1: Extracting text from image...\")\n",
    "            extracted_text = self.extractor.extract_text_from_file(file_path)\n",
    "            results[\"extracted_text\"] = extracted_text\n",
    "\n",
    "            # Step 2: Analyze blood results\n",
    "            print(\"üß™ Step 2: Analyzing blood test results...\")\n",
    "            analysis = self.analyzer.analyze_blood_results(\n",
    "                extracted_text, analysis_mode\n",
    "            )\n",
    "            results[\"analysis\"] = analysis\n",
    "\n",
    "            # Step 3: Generate summary (optional)\n",
    "            if generate_summary:\n",
    "                print(\"üìã Step 3: Generating health summary...\")\n",
    "                summary = self.analyzer.generate_summary(analysis)\n",
    "                results[\"summary\"] = summary\n",
    "\n",
    "            print(\"‚úÖ Pipeline completed successfully\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pipeline failed: {e}\")\n",
    "            results[\"error\"] = str(e)\n",
    "            return results\n",
    "        finally:\n",
    "            # Clean up GPU memory\n",
    "            self.extractor.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed1fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9e8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_hf = AiVetPipeline(\n",
    "#     vision_model_name=\"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     vision_provider=\"huggingface\",\n",
    "#     llm_provider=\"qwen\",\n",
    "#     knowledge_base_path=\"./vet_knowledge_base/\"\n",
    "# )\n",
    "\n",
    "# Example 2: Using Ollama Gemma 3 multimodal model\n",
    "# pipeline_ollama = AiVetPipeline(\n",
    "#     vision_model_name=\"qwen2.5vl:7b\",  # or \"gemma2:27b-vision\" for larger model\n",
    "#     vision_provider=\"ollama\",\n",
    "#     llm_provider=\"ollama\",  # Using Ollama for text analysis too\n",
    "#     llm_model_name=\"qwen2.5:7b\",  # Using Ollama for text analysis too\n",
    "#     knowledge_base_path=\"./vet_knowledge_base/\",\n",
    "#     vision_config={\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"top_k\": 1,\n",
    "#         \"num_ctx\": 32000,  # Take advantage of Gemma 3's 128K context\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Process a blood test image\n",
    "# results = pipeline_ollama.process_blood_test(\n",
    "#     file_path=\"../data/input/202402_hope_emo.pdf\",\n",
    "#     analysis_mode=\"few_shot\",\n",
    "#     generate_summary=True,\n",
    "# )\n",
    "\n",
    "# print(\"üìä Results:\")\n",
    "# for key, value in results.items():\n",
    "#     print(f\"\\n{key.upper()}:\\n{value}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64748a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Here is the extracted information from the veterinary blood test report:\n",
    "\n",
    "1. **RBC (Red Blood Cells)**\n",
    "   - Detected Level: 7.18\n",
    "   - Normal Range: 5.90 - 8.10 10^6/¬µl\n",
    "   - Status: Normal\n",
    "\n",
    "2. **HB (Hemoglobin)**\n",
    "   - Detected Level: 15.6\n",
    "   - Normal Range: 13.1 - 18.9 g/dl\n",
    "   - Status: Normal\n",
    "\n",
    "3. **HCT (Hematocrit)**\n",
    "   - Detected Level: 44.6\n",
    "   - Normal Range: 38.6 - 54.5 %\n",
    "   - Status: Normal\n",
    "\n",
    "4. **MCV (Mean Corpuscular Volume)**\n",
    "   - Detected Level: 62.1\n",
    "   - Normal Range: 61 - 72.6 fL\n",
    "   - Status: Normal\n",
    "\n",
    "5. **MCH (Mean Corpuscular Hemoglobin)**\n",
    "   - Detected Level: 22.0\n",
    "   - Normal Range: 20 - 26 pg\n",
    "   - Status: Normal\n",
    "\n",
    "6. **MCHC (Mean Corpuscular Hemoglobin Concentration)**\n",
    "   - Detected Level: 35.00\n",
    "   - Normal Range: 30 - 37 g/dl\n",
    "   - Status: Normal\n",
    "\n",
    "7. **CH (Cholesterol)**\n",
    "   - Detected Level: 21.1\n",
    "   - Normal Range: 21.0 - 26.5 mg/dl\n",
    "   - Status: Normal\n",
    "\n",
    "8. **CHDW (Cholesterol Distribution Width)**\n",
    "   - Detected Level: 2.79\n",
    "   - Normal Range: 1.7 - 3.80 mg/dl\n",
    "   - Status: Normal\n",
    "\n",
    "9. **HDW (Hemoglobin Distribution Width)**\n",
    "   - Detected Level: 1.7\n",
    "   - Normal Range: 1.60 - 2.27 g/dl\n",
    "   - Status: Normal\n",
    "\n",
    "10. **RDW (Red Cell Distribution Width)**\n",
    "    - Detected Level: 13.8\n",
    "    - Normal Range: 10.9 - 14.5 %\n",
    "    - Status: Normal\n",
    "\n",
    "11. **WBC (White Blood Cells)**\n",
    "    - Detected Level: 13.88\n",
    "    - Normal Range: 5.70 - 13.80 10^3/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "12. **Neutrophils (Neut)**\n",
    "    - Detected Level: 11381.6\n",
    "    - Normal Range: 3800 - 8800/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "13. **Lymphocytes (Linf)**\n",
    "    - Detected Level: 1665.6\n",
    "    - Normal Range: 1300 - 4100/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "14. **Monocytes (Mono)**\n",
    "    - Detected Level: 694.0\n",
    "    - Normal Range: 200 - 740/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "15. **Eosinophils (Eos)**\n",
    "    - Detected Level: 138.8\n",
    "    - Normal Range: 150 - 1100/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "16. **Basophils (Baso)**\n",
    "    - Detected Level: 0\n",
    "    - Normal Range: 0 - 100/¬µl\n",
    "    - Status: Normal\n",
    "\n",
    "17. **Platelets (PLT)**\n",
    "    - Detected Level: 101\n",
    "    - Normal Range: 150 - 460 1000/¬µl\n",
    "    - Status: Low\n",
    "\n",
    "18. **MPV (Mean Platelet Volume)**\n",
    "    - Detected Level: 9.7\n",
    "    - Normal Range: 8.6 - 18 fL\n",
    "    - Status: Normal\n",
    "\n",
    "19. **PCT (Platelet Crit)**\n",
    "    - Detected Level: 0.10\n",
    "    - Normal Range: 0.24 - 0.70\n",
    "    - Status: Low\n",
    "\n",
    "20. **MPC (Mean Platelet Count)**\n",
    "    - Detected Level: 26.20\n",
    "    - Normal Range: 53 - 70\n",
    "    - Status: Normal\n",
    "\n",
    "21. **MPD (Mean Platelet Distribution Width)**\n",
    "    - Detected Level: 57.50\n",
    "    - Normal Range: 53 - 70 %\n",
    "    - Status: Normal\n",
    "\n",
    "22. **LPLT (Low Platelet Level)**\n",
    "    - Detected Level: 4\n",
    "    - Normal Range: 1 - 50 1000/¬µl\n",
    "    - Status: Low\n",
    "\n",
    "23. **Plasma Opalescence**\n",
    "    - Detected Level: Plasma opalescent\n",
    "    - Status: Normal\n",
    "--------------------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7688efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "class LocalVectorStore:\n",
    "    \"\"\"\n",
    "    Encapsulates a persistent Chroma client and collection with consistent settings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Class-level settings to ensure consistency\n",
    "    CHROMA_SETTINGS = Settings(\n",
    "        allow_reset=True,\n",
    "        anonymized_telemetry=False,\n",
    "        is_persistent=True\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self, persist_path: str = \"./chroma_db\", collection_name: str = \"vet_docs\"\n",
    "    ):\n",
    "        self.persist_path = persist_path\n",
    "        self.collection_name = collection_name\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        \n",
    "        # Initialize with consistent settings\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize ChromaDB client with consistent settings.\"\"\"\n",
    "        try:\n",
    "            # Always use the same settings for consistency\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=self.persist_path, \n",
    "                settings=self.CHROMA_SETTINGS\n",
    "            )\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name\n",
    "            )\n",
    "            print(f\"‚úÖ Successfully connected to ChromaDB at: {self.persist_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"readonly\" in str(e).lower():\n",
    "                print(f\"‚ö†Ô∏è Database readonly error - likely settings mismatch: {e}\")\n",
    "                print(\"üîß This usually means the DB was created with different settings\")\n",
    "                raise RuntimeError(\n",
    "                    f\"Database settings mismatch. Please delete {self.persist_path} \"\n",
    "                    \"and run with overwrite=True to recreate with consistent settings.\"\n",
    "                )\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Properly reset the ChromaDB client and delete all data.\"\"\"\n",
    "        print(\"üîÑ Resetting ChromaDB...\")\n",
    "        \n",
    "        # Step 1: Properly close/reset the client if it exists\n",
    "        if self.client is not None:\n",
    "            try:\n",
    "                # Try to delete the collection first\n",
    "                if self.collection is not None:\n",
    "                    try:\n",
    "                        self.client.delete_collection(name=self.collection_name)\n",
    "                        print(f\"üóëÔ∏è Deleted collection: {self.collection_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Could not delete collection: {e}\")\n",
    "                \n",
    "                # Reset the client if possible\n",
    "                try:\n",
    "                    self.client.reset()\n",
    "                    print(\"üîÑ Reset ChromaDB client\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Could not reset client: {e}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error during client cleanup: {e}\")\n",
    "            finally:\n",
    "                # Clear references\n",
    "                self.client = None\n",
    "                self.collection = None\n",
    "        \n",
    "        # Step 2: Remove the entire directory\n",
    "        if os.path.exists(self.persist_path):\n",
    "            try:\n",
    "                shutil.rmtree(self.persist_path)\n",
    "                print(f\"üóëÔ∏è Deleted DB directory: {self.persist_path}\")\n",
    "            except PermissionError as e:\n",
    "                print(f\"‚ùå Permission error: {e}\")\n",
    "                # Try to fix permissions and retry\n",
    "                try:\n",
    "                    for root, dirs, files in os.walk(self.persist_path):\n",
    "                        for d in dirs:\n",
    "                            os.chmod(os.path.join(root, d), 0o777)\n",
    "                        for f in files:\n",
    "                            os.chmod(os.path.join(root, f), 0o777)\n",
    "                    shutil.rmtree(self.persist_path)\n",
    "                    print(f\"üóëÔ∏è Successfully deleted DB after permission fix\")\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(f\"Cannot delete database directory: {e2}\")\n",
    "        \n",
    "        # Step 3: Reinitialize with fresh client\n",
    "        self._initialize_client()\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadatas: Optional[List[dict]] = None,\n",
    "    ):\n",
    "        \"\"\"Add documents to the collection.\"\"\"\n",
    "        if not self.collection:\n",
    "            raise RuntimeError(\"Collection not initialized\")\n",
    "            \n",
    "        ids = [f\"doc-{i}\" for i in range(len(texts))]\n",
    "        \n",
    "        self.collection.add(\n",
    "            documents=texts,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas or [{} for _ in texts],\n",
    "            ids=ids,\n",
    "        )\n",
    "\n",
    "    def query(self, query_embedding: List[float], top_k: int = 5):\n",
    "        \"\"\"Query the collection.\"\"\"\n",
    "        if not self.collection:\n",
    "            raise RuntimeError(\"Collection not initialized\")\n",
    "            \n",
    "        return self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "        )\n",
    "\n",
    "\n",
    "class VetVectorStoreBuilder:\n",
    "    \"\"\"Builds and manages a veterinary domain vector store with Chroma and HuggingFace embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = embedding_model\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.embedding_model,\n",
    "            model_kwargs={\"device\": device},\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def sanitize_metadata(metadata: dict) -> dict:\n",
    "        \"\"\"Sanitize metadata to ensure ChromaDB compatibility.\"\"\"\n",
    "        simple_types = (str, int, float, bool, type(None))\n",
    "        sanitized = {}\n",
    "        for k, v in metadata.items():\n",
    "            if isinstance(v, simple_types):\n",
    "                sanitized[k] = v\n",
    "            else:\n",
    "                sanitized[k] = str(v)\n",
    "        return sanitized\n",
    "\n",
    "    def load_documents(self, docs_path: str) -> List:\n",
    "        \"\"\"Load documents from the specified path.\"\"\"\n",
    "        documents = []\n",
    "        if os.path.isdir(docs_path):\n",
    "            for filename in os.listdir(docs_path):\n",
    "                filepath = os.path.join(docs_path, filename)\n",
    "                try:\n",
    "                    if filename.endswith(\".pdf\"):\n",
    "                        loader = PyPDFLoader(filepath)\n",
    "                        loaded_docs = loader.load()\n",
    "                        if not loaded_docs or all(\n",
    "                            not d.page_content.strip() for d in loaded_docs\n",
    "                        ):\n",
    "                            raise ValueError(\"Fallback to OCR\")\n",
    "                    elif filename.endswith(\".txt\"):\n",
    "                        loader = TextLoader(filepath)\n",
    "                        loaded_docs = loader.load()\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Skipping unsupported file type: {filename}\")\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    print(f\"‚ö†Ô∏è Falling back to OCR for: {filepath}\")\n",
    "                    loader = UnstructuredPDFLoader(filepath, mode=\"elements\")\n",
    "                    loaded_docs = loader.load()\n",
    "\n",
    "                print(f\"üìÑ Loaded {len(loaded_docs)} docs from {filename}\")\n",
    "                documents.extend(loaded_docs)\n",
    "        else:\n",
    "            raise ValueError(f\"Documents path is not a directory: {docs_path}\")\n",
    "\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents loaded from the provided path\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def build_vectorstore(\n",
    "        self, docs_path: str, persist_path: str = \"./chroma_db\", overwrite: bool = False\n",
    "    ):\n",
    "        \"\"\"Build the vector store with improved error handling.\"\"\"\n",
    "        print(f\"üöÄ Starting vector store build...\")\n",
    "        print(f\"üìÅ Documents path: {docs_path}\")\n",
    "        print(f\"üíæ Persist path: {persist_path}\")\n",
    "        print(f\"üîÑ Overwrite mode: {overwrite}\")\n",
    "        \n",
    "        # If overwrite is True, delete the directory BEFORE creating the vector store\n",
    "        if overwrite and os.path.exists(persist_path):\n",
    "            print(\"üîÑ Overwrite mode: deleting existing database...\")\n",
    "            shutil.rmtree(persist_path)\n",
    "            print(f\"üóëÔ∏è Deleted existing DB at: {persist_path}\")\n",
    "        \n",
    "        documents = self.load_documents(docs_path)\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(documents)\n",
    "        if not splits:\n",
    "            splits = [doc for doc in documents if doc.page_content.strip()]\n",
    "\n",
    "        texts = [doc.page_content for doc in splits]\n",
    "        metadatas = [self.sanitize_metadata(doc.metadata) for doc in splits]\n",
    "\n",
    "        print(f\"üß† Preparing to embed {len(texts)} text chunks...\")\n",
    "\n",
    "        # Initialize vector store AFTER potential deletion\n",
    "        vector_store = LocalVectorStore(persist_path=persist_path)\n",
    "\n",
    "        # Generate embeddings\n",
    "        print(\"üî¢ Generating embeddings...\")\n",
    "        embeddings = self.embeddings.embed_documents(texts)\n",
    "        \n",
    "        # Add documents to vector store\n",
    "        print(\"üíæ Adding documents to vector store...\")\n",
    "        vector_store.add_documents(texts, embeddings, metadatas=metadatas)\n",
    "\n",
    "        print(f\"‚úÖ Vector store created at: {persist_path}\")\n",
    "        print(f\"üìä Total documents indexed: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ac32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import py7zr\n",
    "\n",
    "# archive_path = \"../vetdb/Veterinary Hematology, A Diagnostic Guide and Color Atlas (VetBooks.ir).7z\"\n",
    "# extract_path = \"../vetdb\"\n",
    "# password = \"vetbooks.ir\"\n",
    "\n",
    "# with py7zr.SevenZipFile(archive_path, mode='r', password=password) as archive:\n",
    "#     archive.extractall(path=extract_path)\n",
    "\n",
    "# print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "740016a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./chroma_db\"):\n",
    "    shutil.rmtree(\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17193ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting vector store build...\n",
      "üìÅ Documents path: ../vetdb\n",
      "üíæ Persist path: ./chroma_db\n",
      "üîÑ Overwrite mode: True\n",
      "‚ö†Ô∏è Falling back to OCR for: ../vetdb/Understanding-Blood-Results-A-Pet-Owners-Guide.pdf\n",
      "üìÑ Loaded 80 docs from Understanding-Blood-Results-A-Pet-Owners-Guide.pdf\n",
      "üß† Preparing to embed 80 text chunks...\n",
      "‚úÖ Successfully connected to ChromaDB at: ./chroma_db\n",
      "üî¢ Generating embeddings...\n",
      "üíæ Adding documents to vector store...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m builder = VetVectorStoreBuilder()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_vectorstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../vetdb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# folder with PDFs/text\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# where to store Chroma vectors\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# set False to append\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 234\u001b[39m, in \u001b[36mVetVectorStoreBuilder.build_vectorstore\u001b[39m\u001b[34m(self, docs_path, persist_path, overwrite)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Add documents to vector store\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müíæ Adding documents to vector store...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Vector store created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Total documents indexed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mLocalVectorStore.add_documents\u001b[39m\u001b[34m(self, texts, embeddings, metadatas)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCollection not initialized\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m ids = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoc-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/chromadb/api/models/Collection.py:89\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m     81\u001b[39m     ids=ids,\n\u001b[32m     82\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     uris=uris,\n\u001b[32m     87\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/aivet/lib/python3.11/site-packages/chromadb/api/rust.py:407\u001b[39m, in \u001b[36mRustBindingsAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add\u001b[39m(\n\u001b[32m    387\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    396\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    398\u001b[39m         CollectionAddEvent(\n\u001b[32m    399\u001b[39m             collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m         )\n\u001b[32m    405\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "builder = VetVectorStoreBuilder()\n",
    "builder.build_vectorstore(\n",
    "    docs_path=\"../vetdb\",  # folder with PDFs/text\n",
    "    persist_path=\"./chroma_db\",  # where to store Chroma vectors\n",
    "    overwrite=True,  # set False to append\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09732802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulated blood test result ---\n",
    "blood_results = context\n",
    "# --- Setup: With RAG ---\n",
    "print(\"üîç Testing with RAG enabled...\")\n",
    "analyzer_with_rag = VeterinaryTextAnalyzer(\n",
    "    llm_provider=\"ollama\",\n",
    "    llm_model_name=\"qwen2.5:7b\",\n",
    "    knowledge_base_path=\"../vetdb\",  # Path to your veterinary knowledge base (folder or file)\n",
    ")\n",
    "\n",
    "analysis_with_rag = analyzer_with_rag.analyze_blood_results(\n",
    "    blood_results, mode=\"few_shot\"\n",
    ")\n",
    "summary_with_rag = analyzer_with_rag.generate_summary(analysis_with_rag)\n",
    "\n",
    "print(\"\\nüìä Analysis with RAG:\\n\", analysis_with_rag)\n",
    "print(\"\\nüìù Summary with RAG:\\n\", summary_with_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bf6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: Without RAG ---\n",
    "print(\"\\n\\nüîç Testing WITHOUT RAG...\")\n",
    "analyzer_without_rag = VeterinaryTextAnalyzer(\n",
    "    llm_provider=\"ollama\",\n",
    "    llm_model_name=\"qwen2.5:7b\",\n",
    "    knowledge_base_path=None,  # No RAG support\n",
    ")\n",
    "\n",
    "analysis_without_rag = analyzer_without_rag.analyze_blood_results(\n",
    "    blood_results, mode=\"few_shot\"\n",
    ")\n",
    "summary_without_rag = analyzer_without_rag.generate_summary(analysis_without_rag)\n",
    "\n",
    "print(\"\\nüìä Analysis without RAG:\\n\", analysis_without_rag)\n",
    "print(\"\\nüìù Summary without RAG:\\n\", summary_without_rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
